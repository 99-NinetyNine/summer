{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install -U langgraph langsmith"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install -qU langchain-ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_URL = \"https://ollama.khawasprasiddha.fun/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "llm = ChatOllama(\n",
    "    model=\"gemma3:4b\",\n",
    "    temperature=0,\n",
    "    base_url=BASE_URL,\n",
    "    max_tokens=100,\n",
    "    # other params...\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: Okay, here’s a 500-word essay on Nepal, aiming for a balance of historical, geographical, and cultural elements:\n",
      "\n",
      "---\n",
      "\n",
      "**Nepal: A Land of Giants and Spirituality**\n",
      "\n",
      "Nepal, a small, landlocked nation nestled in the heart of the Himalayas, is a country of breathtaking beauty, profound spirituality, and a resilient people. Often defined by its towering peaks, including the world’s highest, Mount Everest, Nepal offers a travel experience unlike any other, steeped in ancient traditions and a palpable sense of serenity. Its significance extends far beyond its stunning landscapes; it represents a crossroads of cultures, a refuge for devout Buddhists and Hindus, and a nation grappling with the challenges of modernity while fiercely preserving its heritage.\n",
      "\n",
      "Geographically, Nepal is a dramatic masterpiece sculpted by tectonic forces. The High Himalayas dominate the landscape, forming a formidable barrier that has profoundly shaped the country's climate, culture, and economy. The fertile Tarai plains in the south provide agricultural heartland, while the mountains offer unique ecosystems, supporting diverse flora and fauna, including the elusive snow leopard and red panda. The country’s varied topography contributes to a remarkable range of microclimates, from subtropical heat to alpine cold.\n",
      "\n",
      "Historically, Nepal has been a land of kingdoms and empires, with a complex and often isolated past. For centuries, the Shah dynasty ruled, skillfully navigating interactions with the British during the colonial era. Nepal maintained its independence, largely due to its mountainous terrain and strategic location. The Rana dynasty, who preceded the Shahs, further solidified Nepal’s isolation, preserving a unique political and social system. The 1990s witnessed a transition to a democratic republic, marking a pivotal shift in the nation’s trajectory.\n",
      "\n",
      "However, it is Nepal’s cultural richness that truly defines its character. The country is predominantly Hindu and Buddhist, with strong influences from both religions interwoven in daily life. Hinduism, brought by traders and migrants, is practiced by the majority, while Buddhism, introduced by Siddartha Gautama (the Buddha), is deeply rooted in the country's spiritual landscape. Numerous ancient temples and monasteries, like Pashupatinath and Swayambhunath, stand as testaments to this enduring faith. \n",
      "\n",
      "The Sherpa people, hailing from the eastern Himalayas, are renowned for their mountaineering skills and their deep connection to the mountains. Their expertise has been instrumental in the success of countless expeditions to Everest and other Himalayan peaks. Beyond Sherpas, a diverse range of ethnic groups – Newars, Gurungs, Magars, and many more – contribute to Nepal’s vibrant cultural mosaic.\n",
      "\n",
      "In recent decades, Nepal has opened itself to tourism, recognizing its potential as a unique destination. While"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 17\u001b[39m\n\u001b[32m     15\u001b[39m response.raise_for_status()\n\u001b[32m     16\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mResponse:\u001b[39m\u001b[33m\"\u001b[39m, end=\u001b[33m\"\u001b[39m\u001b[33m \u001b[39m\u001b[33m\"\u001b[39m, flush=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mline\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m.\u001b[49m\u001b[43miter_lines\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     18\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mline\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     19\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mjson\u001b[49m\u001b[43m.\u001b[49m\u001b[43mloads\u001b[49m\u001b[43m(\u001b[49m\u001b[43mline\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mutf-8\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ai/ai/venv/lib/python3.12/site-packages/requests/models.py:869\u001b[39m, in \u001b[36mResponse.iter_lines\u001b[39m\u001b[34m(self, chunk_size, decode_unicode, delimiter)\u001b[39m\n\u001b[32m    860\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Iterates over the response data, one line at a time.  When\u001b[39;00m\n\u001b[32m    861\u001b[39m \u001b[33;03mstream=True is set on the request, this avoids reading the\u001b[39;00m\n\u001b[32m    862\u001b[39m \u001b[33;03mcontent at once into memory for large responses.\u001b[39;00m\n\u001b[32m    863\u001b[39m \n\u001b[32m    864\u001b[39m \u001b[33;03m.. note:: This method is not reentrant safe.\u001b[39;00m\n\u001b[32m    865\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    867\u001b[39m pending = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m869\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43miter_content\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    870\u001b[39m \u001b[43m    \u001b[49m\u001b[43mchunk_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunk_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecode_unicode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecode_unicode\u001b[49m\n\u001b[32m    871\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    872\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpending\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m:\u001b[49m\n\u001b[32m    873\u001b[39m \u001b[43m        \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mpending\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ai/ai/venv/lib/python3.12/site-packages/requests/models.py:820\u001b[39m, in \u001b[36mResponse.iter_content.<locals>.generate\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    818\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m.raw, \u001b[33m\"\u001b[39m\u001b[33mstream\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    819\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m820\u001b[39m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m.raw.stream(chunk_size, decode_content=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m    821\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m ProtocolError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    822\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m ChunkedEncodingError(e)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ai/ai/venv/lib/python3.12/site-packages/urllib3/response.py:1063\u001b[39m, in \u001b[36mHTTPResponse.stream\u001b[39m\u001b[34m(self, amt, decode_content)\u001b[39m\n\u001b[32m   1047\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1048\u001b[39m \u001b[33;03mA generator wrapper for the read() method. A call will block until\u001b[39;00m\n\u001b[32m   1049\u001b[39m \u001b[33;03m``amt`` bytes have been read from the connection or until the\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1060\u001b[39m \u001b[33;03m    'content-encoding' header.\u001b[39;00m\n\u001b[32m   1061\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1062\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.chunked \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.supports_chunked_reads():\n\u001b[32m-> \u001b[39m\u001b[32m1063\u001b[39m     \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m.read_chunked(amt, decode_content=decode_content)\n\u001b[32m   1064\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1065\u001b[39m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_fp_closed(\u001b[38;5;28mself\u001b[39m._fp) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m._decoded_buffer) > \u001b[32m0\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ai/ai/venv/lib/python3.12/site-packages/urllib3/response.py:1219\u001b[39m, in \u001b[36mHTTPResponse.read_chunked\u001b[39m\u001b[34m(self, amt, decode_content)\u001b[39m\n\u001b[32m   1216\u001b[39m     amt = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1218\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1219\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_update_chunk_length\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1220\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.chunk_left == \u001b[32m0\u001b[39m:\n\u001b[32m   1221\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ai/ai/venv/lib/python3.12/site-packages/urllib3/response.py:1138\u001b[39m, in \u001b[36mHTTPResponse._update_chunk_length\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1136\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.chunk_left \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1137\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1138\u001b[39m line = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_fp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[union-attr]\u001b[39;00m\n\u001b[32m   1139\u001b[39m line = line.split(\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m;\u001b[39m\u001b[33m\"\u001b[39m, \u001b[32m1\u001b[39m)[\u001b[32m0\u001b[39m]\n\u001b[32m   1140\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/socket.py:707\u001b[39m, in \u001b[36mSocketIO.readinto\u001b[39m\u001b[34m(self, b)\u001b[39m\n\u001b[32m    705\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m    706\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m707\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sock\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    708\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[32m    709\u001b[39m         \u001b[38;5;28mself\u001b[39m._timeout_occurred = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "## time\n",
    "import requests\n",
    "import json\n",
    "\n",
    "base_url = \"http://ollama.khawasprasiddha.fun\"\n",
    "endpoint = \"/api/generate\"\n",
    "\n",
    "payload = {\n",
    "    \"model\": \"gemma3:4b\",\n",
    "    \"prompt\": \"write me a 500 word essay on nepal\"\n",
    "}\n",
    "\n",
    "try:\n",
    "    with requests.post(base_url + endpoint, json=payload, stream=True) as response:\n",
    "        response.raise_for_status()\n",
    "        print(\"Response:\", end=\" \", flush=True)\n",
    "        for line in response.iter_lines():\n",
    "            if line:\n",
    "                data = json.loads(line.decode('utf-8'))\n",
    "                print(data.get(\"response\", \"\"), end=\"\", flush=True)\n",
    "        print()  # Newline at the end\n",
    "except requests.exceptions.RequestException as e:\n",
    "    print(\"Error:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for chunk in llm.stream(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": \"what is the weather in sf\"}]},\n",
    "    stream_mode=\"updates\"\n",
    "):\n",
    "    print(chunk)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TypedDict\n",
    "from langgraph.graph import StateGraph, START, END\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class State(TypedDict):\n",
    "  topic: str\n",
    "  joke: str\n",
    "\n",
    "\n",
    "def refine_topic(state: State):\n",
    "    return {\"topic\": state[\"topic\"] + \" and cats\"}\n",
    "\n",
    "\n",
    "def generate_joke(state: State):\n",
    "    llm.invoke(\n",
    "        {\"messages\": [{\"role\": \"user\", \"content\": f\"Write a joke about {state['topic']}\"}]},\n",
    "        stream_mode=\"updates\"\n",
    "    )\n",
    "    return {\"joke\": f\"This is a joke about {state['topic']}\"}\n",
    "\n",
    "graph = (\n",
    "  StateGraph(State)\n",
    "  .add_node(refine_topic)\n",
    "  .add_node(generate_joke)\n",
    "  .add_edge(START, \"refine_topic\")\n",
    "  .add_edge(\"refine_topic\", \"generate_joke\")\n",
    "  .add_edge(\"generate_joke\", END)\n",
    "  .compile()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAJMAAAFNCAIAAACLxMqpAAAAAXNSR0IArs4c6QAAIABJREFUeJztnXlcVFXfwM/sG8PMsO/rIIuKIENiai64UEkaqZkbliku1WtFWK6glamkVqbhVi4l2WPL45JLuFcuKPsiICCCAsLMwGx3Zu6def8YHyIdEWnuDGc63w9/MHc553fnO+ece+499x6K0WgECAih2joARA9B5mAFmYMVZA5WkDlYQeZghW7DvDEV0Xxbq1ERmIrQagwAiu4JBbC5NBaXyuHR3HxZbB7NZoFYvz+nasPLryqqi1XSu1p3fzaHR2PzaGwejUKxciA9wWgEmIrAVIRGRTTewly8WIF9eWGxjjyBtRVa21zuKdm1HFlABDdkID+oH8+aWVscQm+sLVNXXlfcKlfFxDtJxoismbv1zN2txk7ub/QIYA8e7+LoZMta2uK0tegvHWttqsPGTPfwDGRbJ1MrmSu93J57UpqQ7Onmx7JCdjah6RZ2fE/jU+Ocwgc5WiE7a5i7+EtLS4P22dmeLK6dn8piKsPxPXddvFlDJ7iQnRfp5q6elMqb9WNmuJOaS6/i5P4mJ3cm2c0euYWgtkRVU6yKf+VfpA0AED/VrbpYWV2kIjUXEs1plMTvh1teSPGi2qzPYxtodEriXK8/Drdo1QbyciHR3J9HW4dOcLVhX9WGcBxoT7/g8uexVvKyIMtcS4O29Y7WP5xLUvq9n6B+vKZbmLRRR1L6ZJnLOyMfPJ7086tezuDnnfPOyElKnBRzBgI012M+IRwyEocIvzBuw02NkZzGjhRzt8pVXkHW1padnZ2RkdGDHUeMGNHY2EhCRAAA4B3MqbuhJiNlUsxV5Sv9Qq3dwpWVlfVgr4aGBqVSSUI49/EN5VQVkJI+KdcPm29jktFOZKQMAKiurs7Kyrpy5QqLxerXr19ycnL//v3nzp2bl5cHADh8+HB2drZYLM7Ozr548WJxcTGbzY6NjV24cKGnpycAIDU1lc1mR0RE7Nq1a/r06Vu3bgUAjB8/Pj4+ft26dRaP1smDdT1HZvFkySpzmMpA0oUuDMPmzZtHEMSOHTs2btxoNBoXL16s1+t37NgRERGRmJiYm5srFovz8vIyMzOjo6MzMzPT09MbGho6KlImk1lZWXnlypX09PSkpKRNmzYBAI4cOUKGNgAAm0vFyOnVkVLmNEqC60BKN66urk4ul7/66qtisRgAsG7duvz8fL1ez2AwOm8WGRn5/fff+/v70+l0AIBarU5LS9NqtSwWy1RD7tu3j8lkkhHhA7C4NK2aICNlUsxRacBgMFJplr9V6ufnJxQK09PTn3vuuZiYmMjISIlE8vBmNBrt9u3bmZmZJSUlavX9E4SWlhZvb28AgFgsto420/UUkq4Lk1KnOQjoyjZSfmhsNnvnzp1Dhgz59ttvX3vttaSkpJMnTz682dmzZ1NTUwcMGLB79+7c3NzNmzd3rKJQKFbTBgBQSPVcPjnFg4xEOXy6RoGTkTIAICAgYPHixUeOHMnMzAwMDFy6dOnNmzcf2Obnn3+WSCTz5883Vart7e0dq4xGozWHAagVBM+RlIaDFHNcB1rLHVKu+tTW1h4+fNhU+EaMGLF27VoAQHl5uakwdWzW1tbm5PTXyW1OTs6jEqSQPPqlpUHL5cNjzt2ffauclHsccrk8IyPjiy++qK+vr6qq2r17N4VCiYyMBAB4e3sXFRXl5ubK5XKxWHzlypX8/Hwcx/fu3WuqHs12t318fAAAJ0+eLC0tJSPgW+Vqd39SxjeQYi5Uwq8rVxtIOBmOiopaunTp4cOHJ06cOHXq1JKSku3bt/v6+gIAkpKSDAbDokWLqqurFy1aFBsb++abbw4ePFgqlaanp4eEhKSkpJw7d+6BBAMCAhISErZu3Wrq2FkWowHUV6r7DORbPGUS74lnZ9YNHCUiKWhYKL+qKLwon/K2LxmJk3WvIGqE6PKvUqMBitGvpGAwGK8cb40eQdaYBrJGz4VJ+NdzZBXXlaES88XurbfeKiwsfHg5QRBGo9HUg36Yo0eP8nikjNLMz89fvHix2VUEQdBojzzLOHPmjNnTnLIrCiaHGhLtYNEw/4LEEUT1lZoTexunpvqZHf+rVqsJwnyfD8fxR5nj80msfhUKRQ/2MhuSUo4f2FCXONfLI4Cs4Zfkjv26+HNLQ5Vm0mIfGh2GoecWAtcZDm6qD4jgPZ3oTF4u5I79GjrRhSug5WQ3k5pLbyMnu1ngzCBVmzWewkqY5dneqj+y8y6us/+zFb3WeGTHHYUMHzvLg+y8rDHGmcCNJ/c3ypr0ifM8+SJGN/aAEoVM/8tXd9x8WPGvuFuhdbDeEyHXT8uu/SaTjHEa8IzQzkZgErgx/5z8Wo4sJl4UE2+lJ3qs+hSWtFF3LUfWWIsNeEboLeY4e1rvmj1JtNzRNVSpC87JvYI4MWOcRG7Wq1Fs8OSjQoZXXFPUlKhkTTqPALbQjSl0ZQhdmVQYHhcxGID8nk7erJff092twZw9mQF9eX0G8vkiaz9XZgNzHWiUxN1aTN6sk9/Tt0v1Bkvf0auoqOjTp49l06TSgMCJIXBliNyYnoHsf9fTxlZDIpHk5ubaOgqygKGGQpgDmYMVZA5WkDlYQeZgBZmDFWQOVpA5WEHmYAWZgxVkDlaQOVhB5mAFmYMVZA5WkDlYQeZgBZmDFWQOVpA5WEHmYAWZgxVkDlaQOVhB5mAFmYMVZA5WkDlYQeZgBZmDFWQOVpA5WEHmYAWZgxVkDlaQOVhB5mAFmYMVZA5WkDlYscM32YwdO5bBYFAolDt37nh4eFAoFIPB8Ouvv9o6Lgtj7bdVWYGWlhYqlQoAoFKpzc3NAAADGS/gtzV2WFs+9dRTnVUZDIa4uDibRkQKdmhu1qxZItFfL5kUCoXTp0+3aUSkYIfmnn76adNESibCwsKGDBli04hIwQ7NAQBmzpwpEAgAAI6OjnZZ4OzW3NChQ03FLjQ01C4LnAXOLXWYoaVB2wt7Fi8mzGlrpE0c92pDlcbWsTwIhQJcvFlM9j8qNj3vz924prh+Wq7DDDxHGgD/ookkLIFR1Y6zOLTokcLQmB7OetJDcz9vazAQYMgEDy45Exr+G1C34xd/aaLTwIQF3j3YvScF9lqODFMZRs/wQtr+CVxH+pgZXhq1Ie+MvAe798Rc4Xn5kAnuZM9z+W+AQqHEPe9WcN4q5rQag15vFLpBP9VAL8HZk4WpCf2TT33zxOYUUr0dz9BiExydmW0t+ifd64nN2ePFWxtDpYAeTI5pnz3xfwPIHKwgc7CCzMEKMgcryBysIHOwgszBCjIHK8gcrCBzsNLrzO3dt3PSlITECSMAAC9MGLn/2922jsg8K1ampi15w4YB9K4xzhiGff3NV88mvJAwLhEAMHVqcr++A8jI6NCP2VVVN5akrepxCsOHjyZw3KJBPRm9y5xGowYAxMUNjYyMBgBMe2U2SRlVVJZRKf+ovhkdn2C5cHqCNcytWJnKYrPDQiP27d/1wZKMuLihxcUF3+zJunGj1MnZJW7Q0NnJKRwO52ruJVP9syo9jcPhHDty4YUJI6dMmTlj+muHfszO/n5P5vqtK1al3r59Kzg4ZOrLyR3f3a/H//vfw4dqa28GBYXEj0pIevHlruP5v7fnFhbmAQCOnzi8a0d2UJC4rq520+a1FZVlDAYzICBozqsL+/ePAgB8sGwxh83x8fHL/n6v0WgUB/dZkpYeEBBkOiitFlu/bgsAoK29bdu2TSdOHhEIhLGxg1PmvuXi4kr2t2qNdo7JZFZXV167fuX9tPTw8H719XXvLVmEE/jWL/esXL62/EZJatpCg8EQK4k79MMJAED6qnXHjlx4IAWFov3zL9Z/sCTjTE5u3KCh69any2RSAMCp335dv2F1eFjfA98enp2c8t2Br7O2f951PJ9t2hEWGpEwLvFMTm5QkFgmk77x5qve3r67dx78bNMOBwf+mo+W6nQ6AACTwbx2/Qqdzjh5/M9dO7IdHQWrMtIeGHOl1+s/WPp/CmX7xk+/WrTgnTt36j9Y+n8EYenJ0R/CSmcod+82ZKxaP3jwMIFAeOq3Y0wmK2PVel9f/+DgkPfeXVFaWnT58u9d7E6hULRa7etzFoWH9wMAPPvsBBzHq25WAACOHvspOkry1ptpQqEoVhI3OznlP4e+a1e0dz+27w/uY3M477y91N3dw98/cElaukwmPXL0JwCA0WjU6bSmStvPLyB51ry6utqy8pLOu1+6fLGsrHjB/LejoyRjxjy3aME7AYHBcrnsH3xb3cJK5gIDxUzm/aErpaVFYWF9BQKh6aOPj5+bm3tefldTtpt+5qGhEaaPDjwHAIBSqTAYDCUlhRLJX4/qREfH4jheVJjX/dhqam/26RNuenALAMB34Ht5+VRUlpk+Bgf3odPvtyne3r4AgJqaqs67V1dXOfAcfLx9TR8jIvov+2CNs7NL9wPoGdZo5ygUSoc20zdefqN0ZLyk8zZSaUsXKZjMPTzaTKfT4Ti+Y+eWHTu3/C01WWv3w5NJW319/Tsv4XK4GrXalC+HzelYzmZzOk6jOlAo2llsdvezsxTWMGc0Gju3DU7OLpGR0bOTUzpvIxSIzO36GNhsNpfLHTcucdjQkZ2X+3j7dT8RLo+HabHOS9QatanQUCgUdSdPGKYBAHA43M4bOzjwH3BpHWzQEw/wD2puaowaEBMdJTH9CQUiP7+AnqUWGChWqZQdSUWE93d1cXN1dXvMbp2Kb2ifiLKyYvx/nTO5XNbQcDsoKMT0saKiTKFU3P+/styUY+eUwkIj1Gr1jYr7tWt1ddXid+bV1dX27HC6jw3MTZk8Q4/rt27bhGFYTc3NbV9tnpsyrceH+vpriy5cOH3ixBGCIAoL89JXL0lNW6jXP2YQnJend2lZUV5+blubPDHxpfb2to2bPm5tbamurvp47QoHB/7o+GdNZY7Pd/xiywaFUtHW3rZv/05fX//wsL6dk5JI4ry8fLKyPrv4+9mruZc2f/6JXC7z/l+zRx42MCcQCL/e/QOdTp8zd+prr79cVJz//pKMwMDgnqUWFRWTtW3/9fyrLyaNTnv/DS2Gfbh6I4PxmBGh48cnGQyG99IW1dZW+3j7rlr5SVXVjUlTEt59bwGdwdi8cTv7f01XiDjU08Nr8pSEiS/GS6Wtq9M3PNDc0un0zA1bcQI3XQ8TOAo/XP0pjUb6uP0nfiKk+bY2J7t5/DzSf1O9gc7dbfI4uv12/Cturj6sJ9qr111xRnST3nXd0oK8MGHko6qTZUs/jIsbavWILIzdmsvK+vZRq0RCp24msmZ1puUisjB2a87Tw8vWIZALaudgBZmDFWQOVpA5WEHmYAWZgxVkDlaQOVhB5mDlic3R6BQj0fte0AYzBsJIoz/xa4Ge2JzIjdHWqu+FL9eDFAMB5C16oesTvxnoic1RaRSBC6OhQvWkOyLMUl+hcvJgUp/8RmxP2rkhiS6Xf72nVaN32vxTMBVx5Vjz0+Ode7BvD9+SWFui+v1wa+QzTq4+bJ7Abm84kIdSjt+7jRVdlA6b6OIXxu3GHg/S8zeTqtuJazmy+kp1611dz1L4N+PixfQJ4caMFnEcejhixQ7nCOlAIpHk5nY1dBpqUH8OVpA5WEHmYAWZgxVkDlaQOVhB5mAFmYMVZA5WkDlYQeZgBZmDFWQOVpA5WEHmYAWZgxVkDlaQOVhB5mAFmYMVZA5WkDlYQeZgBZmDFWQOVpA5WEHmYAWZgxVkDlaQOVhB5mAFmYMVZA5WkDlYQeZgBZmDFWQOVpA5WEHmYAWZgxVkDlbs8B1EAwcO7Jg01YTBYLh+/brtIiIFOyxzYvHf5mQ0GAwhISG2C4cs7NDc1KlTO0/Iy+FwXnnlFZtGRAp2aC4pKcnX96+JDb29vSdOnGjTiEjBDs0BACZNmsRisQAALBZr2rRptg6HFOzT3OTJk/38/AAAXl5edlng7NYcAOCll16y4wL3+F5BQ5Wm6GLb3VqNqo2wYlT/angCmmcgJ3KowCuY08VmXZm78HNLUy0WHe8idGMy2XZbOnsbOswgb9blnW71DGQPeeGRL+d+pLn8c/I71dphSe5kBonoiguHmnxC2JHDBGbXmi9JqnYi74x80HOuJMeG6IpBz7te+02mUZpvp8ybu1utcfNnoxrStjDZVFdf9t1azOxa826kjTqB8xPPN4KwOEJXZkuD1uwq8+YIwkilPvEcPwiLQ6FSCNz8iQiqD2EFmYMVZA5WkDlYQeZgBZmDFWQOVpA5WEHmYAWZgxVkDlaQOVhB5rqiqqpiZLykuLig681WrExNW/KGtYK6j52bS5wwoqmpsce7i0ROs2a+7uraGwcG2PNU4HfuNiiVyn+SgrOzy6uz51suIktiMXNSaesn61YVlxT4+wclTXy56mZFXt7V7VnfmlZ9ufXT4pICrVY7aNCQWTPnenv5AACqq6vmzJ2a9dX+b/Zk/fnnBXd3j/hRCa/PWUShUAAAxcUF3+zJunGj1MnZJW7Q0NnJKRwOBwBw6NCB73/Y917qyi+3fjo4bljKvLf++OP8mbMnCwqvK5WKfn0HzJg+JzIy+nre1XdTFwAApk4bP/yZ+PRV63Ac37Fzy6XLF1tamiMjBya9ODVWEtf1QVVVVcxNmfbFZ7v69RsAAPj993N79m6vvVUtEjmJxaHvLF7q7OzywC7NzU3zF84cGB27fNlHAIBfj//3v4cP1dbeDAoKiR+VkPTiy5b6wi1WW65bn3779q2Nn2ZlrFp/5typ/PxckwCCIBa/M6+4pCD13RVf7zrI4/IWLko21WAMBgMAkJm5ZtzY8adOXHr3neXfHfjmwsUzAID6+rr3lizCCXzrl3tWLl9bfqMkNW2hwWAAADCYTJVK+cMP+2fNnDt+fBKGYR9/sgLH8Q/eX/3Rh5vc3DyWLX+7XdE+MDr24w83AQCyvzuSvmodAGDzZ5/8+FP2pJemHfjuyJCnhy9b/vYff5zv/gHmXru8Mv29ceMSfzh4fNkHH9bX1235MvOBbZRK5XtLFnl6er+/JAMAcOq3X9dvWB0e1vfAt4dnJ6d8d+DrrO2fW+oLt4w5uVx25eqfU6cmh4VGuLm5p6WuvFVXY1pVWJR3+/atpe+viZXEiURObyxK5XA4P/6UDQAwqR0xYszwZ+LpdHqsJM7Nzb2iogwAcOq3Y0wmK2PVel9f/+DgkPfeXVFaWnT58u+mNNVq9fRpr40aOdbby4fNZu/YfmDx/70fHSWJjpLMm/eWUqUsLS16IEIMw06eOjpj+pzE8UmOfMfxz784/Jn4fft3dv8Yd+3eOmL46JeSpgocBZGR0Qvmv3323G/V1VUdG+A4vmzF2wRBfPzRZjqdDgA4euyn6CjJW2+mCYWiWEnc7OSU/xz6TqPRWOIrt5C5m9WVAID+/aJMH4VCUXR0rOn/4uICNps9YMDA+/lRqf37R+fn5wIATAMGQ0MjOtJxcOArlQoAQGlpUVhYX4FAaFru4+Pn5uael5/bsWVYWN+O/zVq9RdbNkyakjAyXjJh4igAgFTa8kCElZXler3+qdjBHUuioiTlN0pVKlU3j7Gmpqpzpn1CwgAANypKO36CH69dUVNdtX7dFoGjwPT0V0lJoaRThRwdHYvj+O36W93MsWss084pFO0AAC6X17FEIBBKW1sAAEqlAsOwkfGSztubmgeTOdNhm+gY/KlUKspvlD6wl8mHaXvTAx8AgKamxrcWz4mVDF65fG1ERH+CIBKeG/JwhKYfxMI3Zj+wXCpt4fF4D2//0O5KrVbLYrE7lpgOVq1Wmz7mF1zT6/UCgZDFvB+YTqcztaw7dm55OJJ/jmXMsVlsAIBO99coJZm01fSPs7MLj8dbs/rTv+VKe0y+Ts4ukZHRs5NTOi8UCkQddo1Go0nhmbMnCYJYkpbOZrMBADKZ1GyCzi6uAIDUd5d7efl0Xt7NM35T4hj2V0WnVqs6foIAAD7fccWyjzM3fvjJulUb1n9p2oXL5Y4blzhs6MjOSQUH9+lOjo/FMuZ8fP0BADW1N/38AgAA7Yr2gsLr/n6BAIDAQLFKpXJ39/Ty9DZt3HCn3tnpwVOyBwjwDzp9+kTUgJiOEllTcz/xB1Ao2nk8B9M3CwA4e+5Ux6rOpdnL04fJZFIolOio++W4tbWFTqd37Ng1dDo9tE94SUkhmHx/SUlpIQAgOCjE9DMKDgqJiopZsfzjRW/MPvjD/imTZ/zv2JUdOWq12nv3mhz5jt3J8bFYpp3z8fb19fXfu2/HnbsNCqVi8+a1fr73v+VYSVysJC4zc01zc5NcLjv0Y/b8BTNO/Xas6wSnTJ6hx/Vbt23CMKym5ua2rzbPTZlWV1f78JaBgeLW1pYjR3/CcfzPPy+UlRVzOJzm5iYAgKl4nTl7svxGqYODQ/KseXv2bi8tLcIw7MzZU++kzv9iy4buH+OECZPPXzh96MdshVJx7fqVbds2xcUN9fX177xNaJ/w2ckpO3ZuMZ25vP7aogsXTp84cYQgiMLCvPTVS1LTFuI43v1Mu8Bi/bm01JWfbvpoxsyJIeLQsWPHs9mcjtPLT9Z+/tPPBzPWvF9aWuTnF/DcsxMTxyd1nZpAIPx69w/ffff1nLlT79ypDw/v9/6SjMDA4Ie3jB81rqamavfX2z7d+NGgQUPSUlc6Ogr27N2u0agXzF8cH5+wa/fW6CjJ+nVbpr0yOygoZN+3u3JzLwkEwr4RkW8vXtr9A0wYl3jvXnP293u2fJnp4e4pkcS9/rqZK17Tp72ae+3Sqoy0ndsPREXFZG3bv/+73V9u/VSn10WE9/9w9UbTaec/x/wTIX8ebTUaqf2HibqfUFubHMMwd3cP08e0JW/w+Y4rln9skShtRUVlecr8GV9+8XVERH+bBFB4XkalGgY/b+aJHov1xNMzlrybOv/ixbNyueybPVl5+bmJ41+yVOI2oba2+vffzwIAnB7XKtsEi9WW6enrN2Su/mr7Z62t9/z9AtdkZEZFxVgqcfIoKspfumyx2VWYFsNxfOrLszw8PK0e1+OxWG0JL4pHd7D4DnzrxvIgXdSW9nyvoJvYXE/PsPP7c3YMMgcryBysIHOwgszBCjIHK8gcrCBzsGLeHAW9l6HX8KiXZJg35+jMUMj0JIeEeDwKmc7R2fx1LvPmXL1ZTbcsM0QJ8U9oqtW4+pi/a2/enIs3k8unlfwhJzkwRFcUnpdxHekuXuZfBvWodo4ydqZH8UVp/hnzA3IQZJN3urXssuzZ2R6P2qCr91uq2vCT+5saazGhK5PBgu8slCAIGo1m6yieGL3WIL+n8wxkj5nuzhM88mbO42eawFREuxTXaw0kBEkuKSkpWVlZto7iiWGyqXwRnc17zG/u8ffn2DzaY1PpnTS2lXqLu3qhLtTAVwciTCBzsILMwQoyByvIHKwgc7CCzMEKMgcryBysIHOwgszBCjIHK8gcrCBzsILMwQoyByvIHKwgc7CCzMEKMgcryBysIHOwgszBCjIHK8gcrCBzsILMwQoyByvIHKwgc7CCzMEKMgcryBysIHOwgszBCjIHK8gcrCBzsILMwQoyByuPfwcRdERFRVGpf/tFGo3GvLw820VECnZY5sRiMfXvBASYmXIQduzQ3IgRIx5YMnbsWBvFQiJ2aO7ll1/uXMj8/f0nTZpk04hIwQ7Nubq6Dh8+3DTJKoVCGTVqlKurq62Dsjx2aA4AMGXKFD8/P1OBmzJliq3DIQX7NOfu7j5q1CgAwMiRI+2ywPWKXkHdDfXdakzZhmNKg0ZDGAjLJEsQRH19vY+Pj6VeK0ulAQ6HxuHTeI40r2CObx8bvznTZuZa7uhyT8lqS5VsHoMj4tKZNBqDSmfSe+1UCUYjwHU4oTfgOkIjU2MqfUBfB8lo0aPekE02NjCHqYjzP7XWFCud/AQCDwcmB8p5J3UavK1RKb3VFjTAYdhEFzbX2u2Otc1V5KnO/adZ4OHoEuBIpUPfyhK4oaW2rb1RMXKKu3gA15pZW9XclRPSggvtftEeLC7DaplaAUylr7veGBMviIm33pzC1jN3cl9z/U2tX7Q7nQnl+7y7BseIuoJGvxD26Olu1snRSvXV5eOt9dVaf4mnXWoDANDZtIAYr7oq7ZXjVpqbwxrmqouUBefa/SLdabTeeuJoCah0iu8A97xzbTcLldbIjuwMtGpDzoF7vtEedLZ9lrbOMFg0vwHuOdn3MDXpE3OQbu6Po60iHz6Hb5tOj/XhCFgib/6lX0mvM8k119air7yuFPkJSc2lt+HkK7iRq2iX4qTmQq653By5yM+x1zZvB3/+aNPWZIsnS2NQnXwcr50mdw44cs3VFCmdvB1JzaJ3IvLi1xaTe55CormWBq3paiR5WfRa6GwaoFCljToSsyAv6cZbGM+JxAvqV64fvnT1p8amm54eIdGRY4fG3b8Pt3Lt2GdHL2hra/7t3G42ixfeZ8jE59/l8YQAAK1W/e1/VlbevOrt0WdI3GQKhcRfFVfEbryFOXmQdWpGYugKKc7gkHWV63rB8YM/fejrHbH03Z/Hjpp7+vyeIye2mFbRaYwzF/YymewPl51OfTO7sib31NndplUHf/6otbV+4Zxts1755HZD2Y3KSySFBwBgcplKMk9SSDTX1opT6WT14S7l/iIOjHlxfKoDTxQqHjR21NwLfx5Qq9sBAABQ3Fz8Rz2TzGbzhAK3PsFP1d8pBwC0td8rKP5t5LBZvt4RjnznxIS3aDQSqxwajSpvhdNcu1RPY5KSvsFguHW7sI94UMcScZCEIPDqW/kAAACMPl7hHas4bL4GUwAAWqX1AAAP9yDTcgqF4u0ZSkZ4JmhMWnsriTN7k3xvzEDK5Wwc1xEEfuzU1mOntnZerlC2mv6hdLo/23FJXaVuAwAwGH9N8sxkmJ/w2VKQc/T3IdEcj0/HdaRcBGIy2SwmN3bg+H7hfxta6eLs28VeXI4jAECvxzpHnY+KAAADC0lEQVSW6PQkzpmOawkHPokX/Eg0xxXQpFILjSp5CE93sQZTioNiTB/1eq28rUko6OoOi0joAQCoqy/x8QoDAOh0WFV1rkjoRVKEeh0udCXx6yWxnXMQ0PRqsjo0CWPmF5Weyc07ShBEdW3e3u8/2L7nTRzvql1xEnn5+fQ7npPVIq3X67X7f1hOp5N4NRXX6BwEcJY5d392wfk2khIXB8Ysnr8n5/w3vxzbhBM6P59+s6dtoNMf0wmZNinjxyPrN26ZjhP6QTETXCP9btZcIynCtma1u7+ApMTJvSduMBh3LqvxH+jJcvi33CjoQKPQ1eXdnfdxIIW0sWwk1pZUKiV4gIOswRq3GXsb8gZF6EA+edpI7xVEDRce3HTbOUDAYJmv8S/n/nL4xOdmV+F6HZ1hvrBOn7Q6PHSIpYI8fX7P6Qt7za7ish3VWLvZVSmzt/h6h5tdhWOE7I7iuZl+lorQLKSPIMrJbm5pAu59nM2uxTCVWmO+LVRrFFwO3+wqB54Tk2mxrphGozB11R9Gr9cyGCyzq/h8F8YjTnAab7R6eFNGTCZ3VDzp5jRKYs+aW76RbqRefe49qGVYXUHjq6sCWSSPnSX9FgzHgZYwy72h+J4eI6tv13vQY3h9UXPCLA+ytVlp7FdAX96wF50bihoNuL09k94ZA268XdA0YrJLQF+eFbKz3kjZ0svtV062efdzY7ChfJCga/QY3lDcPGicIPwpK40BsOro9Ls12PE9TR5hrhyB+WYfUlQyrLmyJWGWu2cguZewO2PtJ0Lapfgv2xq4Iq7QV0iD/4kQXG+Q18kwBTZxgZeD0Kp1iW2enyu93F70h4LJYzEdODyR9X6nFkQlx3QKDY7p+g/mh8Wa772Qii2fWW29q6vMU9WWqfV6QKVRaHQahU4j9brDP8FoNBpxgsAJg97AZFEC+nHDYhwELjZ7KMn2TxsDAHC9UX5P33ZPJ2/RE3rbx2MWOpMicGYIXJkiVwaNYfufV68wh+gB0J8j/GtB5mAFmYMVZA5WkDlYQeZg5f8BzcHsqZYx41EAAAAASUVORK5CYII=",
      "text/plain": [
       "<langgraph.graph.state.CompiledStateGraph object at 0x7c33eb4bf6e0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'refine_topic': {'topic': 'ice cream and cats'}}\n",
      "{'generate_joke': {'joke': 'This is a joke about ice cream and cats'}}\n"
     ]
    }
   ],
   "source": [
    "for chunk in graph.stream(\n",
    "    {\"topic\": \"ice cream\"},\n",
    "    stream_mode=\"updates\",\n",
    "):\n",
    "    print(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'chatollama'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[28]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdatetime\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m datetime\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# ------------------------------------------------------------------------\u001b[39;00m\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# HIGH-LEVEL IMPORTS (assume these packages exist in your environment)\u001b[39;00m\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# ------------------------------------------------------------------------\u001b[39;00m\n\u001b[32m      8\u001b[39m \n\u001b[32m      9\u001b[39m \u001b[38;5;66;03m# ChatOllamaClient: wraps calls to Gemma 3.4B (or similar) via Ollama\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mchatollama\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ChatOllamaClient\n\u001b[32m     12\u001b[39m \u001b[38;5;66;03m# LangGraphAssistant: a prebuilt LangGraph interface that does:\u001b[39;00m\n\u001b[32m     13\u001b[39m \u001b[38;5;66;03m#  - categorize requests\u001b[39;00m\n\u001b[32m     14\u001b[39m \u001b[38;5;66;03m#  - assess security\u001b[39;00m\n\u001b[32m     15\u001b[39m \u001b[38;5;66;03m#  - generate an action plan (list of ActionNode dataclasses)\u001b[39;00m\n\u001b[32m     16\u001b[39m \u001b[38;5;66;03m#  - (optionally) execute approved actions via an abstract \"toolkit\"\u001b[39;00m\n\u001b[32m     17\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlanggraph\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m LangGraphAssistant, ActionNode, ActionType, SecurityLevel\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'chatollama'"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "import uuid\n",
    "from datetime import datetime\n",
    "\n",
    "# ------------------------------------------------------------------------\n",
    "# HIGH-LEVEL IMPORTS (assume these packages exist in your environment)\n",
    "# ------------------------------------------------------------------------\n",
    "\n",
    "# ChatOllamaClient: wraps calls to Gemma 3.4B (or similar) via Ollama\n",
    "from chatollama import ChatOllamaClient\n",
    "\n",
    "# LangGraphAssistant: a prebuilt LangGraph interface that does:\n",
    "#  - categorize requests\n",
    "#  - assess security\n",
    "#  - generate an action plan (list of ActionNode dataclasses)\n",
    "#  - (optionally) execute approved actions via an abstract \"toolkit\"\n",
    "from langgraph import LangGraphAssistant, ActionNode, ActionType, SecurityLevel\n",
    "\n",
    "# Toolkit: a bundled set of “mock” or real integrations for YouTube, SMS, rides, banking, form-filling, etc.\n",
    "# Each method returns a dict with { \"status\": \"success\"/\"error\", ... }.\n",
    "from toolkit import Toolkit\n",
    "\n",
    "# ------------------------------------------------------------------------\n",
    "# MAIN “APPLICATION” CODE\n",
    "# ------------------------------------------------------------------------\n",
    "\n",
    "class DailyActivityAssistant:\n",
    "    def __init__(self):\n",
    "        # 1) ChatOllamaClient wraps Gemma 3.4B calls; no need to reimplement generate()\n",
    "        self.llm = ChatOllamaClient(model_name=\"gemma:3.4b\")\n",
    "\n",
    "        # 2) LangGraphAssistant encapsulates the entire analysis + plan generation logic.\n",
    "        #    You simply pass user_text and the LLM client, and it returns categories/security/actions.\n",
    "        self.langgraph = LangGraphAssistant(llm_client=self.llm)\n",
    "\n",
    "        # 3) Toolkit holds actual implementations for YouTube, Spotify, SMS, WhatsApp, ride booking, banking, etc.\n",
    "        self.tools = Toolkit()\n",
    "\n",
    "        # In-memory “storage” for conversation logs and action plans (just a dict in this example).\n",
    "        self.memory = {\n",
    "            \"conversation_history\": [],\n",
    "            \"action_plans\": {},       # plan_id → List[ActionNode]\n",
    "            \"execution_history\": []   # logs of executed actions\n",
    "        }\n",
    "\n",
    "    def log_conversation(self, role: str, message: str):\n",
    "        self.memory[\"conversation_history\"].append({\n",
    "            \"timestamp\": datetime.utcnow(),\n",
    "            \"role\": role,\n",
    "            \"message\": message\n",
    "        })\n",
    "\n",
    "    async def run(self):\n",
    "        print(\"🚀 Gemma 3.4B LangGraph Daily Assistant (v2 – abstracted)\")\n",
    "        print(\"=\" * 50)\n",
    "\n",
    "        while True:\n",
    "            user_input = input(\"\\n🎯 Your request (or type 'exit' to quit): \").strip()\n",
    "            if not user_input or user_input.lower() in {\"quit\", \"exit\", \"q\"}:\n",
    "                print(\"👋 Goodbye!\")\n",
    "                return\n",
    "\n",
    "            # Log user message\n",
    "            self.log_conversation(\"user\", user_input)\n",
    "\n",
    "            # ------------------------------------------------------------\n",
    "            # Step 1: ANALYZE + CATEGORIZE + ASSESS SECURITY\n",
    "            # ------------------------------------------------------------\n",
    "            print(\"\\n🔍 Analyzing your request via LangGraphAssistant...\")\n",
    "            analysis = await self.langgraph.analyze(user_input)\n",
    "            # analysis is a dict like:\n",
    "            # {\n",
    "            #   \"category\": ActionType.MEDIA_PLAY,\n",
    "            #   \"security_level\": SecurityLevel.LOW,\n",
    "            #   \"requires_confirmation\": False,\n",
    "            #   \"raw_llm_analysis\": \"...Gemma's text output...\"\n",
    "            # }\n",
    "\n",
    "            print(f\"  • Category:      {analysis['category'].value}\")\n",
    "            print(f\"  • Security (LSL): {analysis['security_level'].value}\")\n",
    "            print(f\"  • Needs confirm: {analysis['requires_confirmation']}\")\n",
    "\n",
    "            # ------------------------------------------------------------\n",
    "            # Step 2: GENERATE a DETAILED ACTION PLAN\n",
    "            # ------------------------------------------------------------\n",
    "            print(\"\\n⚙️  Generating action plan via LangGraphAssistant...\")\n",
    "            plan_id = str(uuid.uuid4())\n",
    "            action_nodes = await self.langgraph.generate_action_plan(user_input, analysis)\n",
    "\n",
    "            # Store the plan\n",
    "            self.memory[\"action_plans\"][plan_id] = action_nodes\n",
    "\n",
    "            # ------------------------------------------------------------\n",
    "            # Step 3: DISPLAY PLAN & HUMAN-IN-THE-LOOP VALIDATION\n",
    "            # ------------------------------------------------------------\n",
    "            self._display_action_plan(action_nodes, plan_id)\n",
    "\n",
    "            approved_actions = self._get_user_approval(action_nodes)\n",
    "            if not approved_actions:\n",
    "                print(\"\\n❌ No actions approved. Returning to main prompt.\")\n",
    "                continue\n",
    "\n",
    "            # ------------------------------------------------------------\n",
    "            # Step 4: EXECUTE APPROVED ACTIONS\n",
    "            # ------------------------------------------------------------\n",
    "            print(f\"\\n🚀 Executing {len(approved_actions)} approved action(s)...\")\n",
    "            for action in approved_actions:\n",
    "                print(f\"\\n▶️  Executing: {action.name} (Type={action.type.value})\")\n",
    "                result = await self._execute_action(action)\n",
    "                timestamp = datetime.utcnow()\n",
    "\n",
    "                # Log execution\n",
    "                self.memory[\"execution_history\"].append({\n",
    "                    \"plan_id\": plan_id,\n",
    "                    \"action_id\": action.id,\n",
    "                    \"action_name\": action.name,\n",
    "                    \"timestamp\": timestamp,\n",
    "                    \"result\": result\n",
    "                })\n",
    "\n",
    "                if result.get(\"status\") == \"success\":\n",
    "                    print(f\"   ✅ {action.name} completed successfully.\")\n",
    "                    if result.get(\"details\"):\n",
    "                        print(f\"      Details: {result['details']}\")\n",
    "                else:\n",
    "                    print(f\"   ❌ {action.name} failed: {result.get('error', '<unknown>')}\")\n",
    "\n",
    "                # (Optional) small pause between actions\n",
    "                await asyncio.sleep(0.3)\n",
    "\n",
    "            print(\"\\n🎉 All approved actions processed!\\n\")\n",
    "\n",
    "    def _display_action_plan(self, actions: list[ActionNode], plan_id: str):\n",
    "        print(\"\\n🤖 Generated Action Plan:\")\n",
    "        print(\"=\" * 60)\n",
    "        for idx, action in enumerate(actions, start=1):\n",
    "            # Simple icons for each security level\n",
    "            sec_icon = {\n",
    "                SecurityLevel.LOW: \"🟢\",\n",
    "                SecurityLevel.MEDIUM: \"🟡\",\n",
    "                SecurityLevel.HIGH: \"🟠\",\n",
    "                SecurityLevel.CRITICAL: \"🔴\"\n",
    "            }[action.security_level]\n",
    "\n",
    "            sens_icon = \"🔒\" if action.sensitive_data else \"🔓\"\n",
    "            conf_icon = \"⚠️\" if action.requires_confirmation else \"✅\"\n",
    "\n",
    "            print(f\"\\n {idx}. {action.name}\")\n",
    "            print(f\"    • Description:   {action.description}\")\n",
    "            print(f\"    • Category:      {action.type.value}\")\n",
    "            print(f\"    • Security:      {sec_icon} {action.security_level.value.upper()}\")\n",
    "            print(f\"    • Sensitive:     {sens_icon} {'Yes' if action.sensitive_data else 'No'}\")\n",
    "            print(f\"    • Confirmation:  {conf_icon} {'Required' if action.requires_confirmation else 'Not Required'}\")\n",
    "            print(f\"    • Parameters:    {action.parameters}\")\n",
    "            if action.dependencies:\n",
    "                print(f\"    • Depends on:    {action.dependencies}\")\n",
    "\n",
    "    def _get_user_approval(self, actions: list[ActionNode]) -> list[ActionNode]:\n",
    "        \"\"\"Walk through each action, ask the user to Approve/Edit/Skip/Quit.\"\"\"\n",
    "        approved = []\n",
    "        print(\"\\n👤 Human-in-the-Loop Validation:\")\n",
    "        for action in actions:\n",
    "            while True:\n",
    "                prompt = (\n",
    "                    f\"\\n🤔 Action: {action.name}\\n\"\n",
    "                    f\"   Description: {action.description}\\n\"\n",
    "                    f\"   Security Level: {action.security_level.value.upper()}\\n\"\n",
    "                )\n",
    "                if action.security_level in (SecurityLevel.HIGH, SecurityLevel.CRITICAL):\n",
    "                    prompt += \"   ⚠️  HIGH-SECURITY operation! Review carefully.\\n\"\n",
    "                if action.sensitive_data:\n",
    "                    prompt += \"   🔒  Contains sensitive data.\\n\"\n",
    "\n",
    "                prompt += \"   [A]pprove / [E]dit / [S]kip / [Q]uit: \"\n",
    "                choice = input(prompt).strip().lower()\n",
    "\n",
    "                if choice == \"a\":\n",
    "                    action.approved = True\n",
    "                    approved.append(action)\n",
    "                    print(\"   ✅ Approved.\")\n",
    "                    break\n",
    "\n",
    "                elif choice == \"e\":\n",
    "                    edited = self._edit_action(action)\n",
    "                    if edited:\n",
    "                        edited.approved = True\n",
    "                        approved.append(edited)\n",
    "                        print(\"   ✅ Edited & Approved.\")\n",
    "                        break\n",
    "\n",
    "                elif choice == \"s\":\n",
    "                    print(\"   ⏭️  Skipped.\")\n",
    "                    break\n",
    "\n",
    "                elif choice == \"q\":\n",
    "                    print(\"   ❌ User requested to quit validation.\")\n",
    "                    return []\n",
    "\n",
    "                else:\n",
    "                    print(\"   ❓ Please enter A, E, S, or Q.\")\n",
    "\n",
    "        return approved\n",
    "\n",
    "    def _edit_action(self, action: ActionNode) -> ActionNode | None:\n",
    "        \"\"\"Allow user to rename or re-describe an action (parameters only if not sensitive).\"\"\"\n",
    "        print(f\"\\n✏️  Editing action '{action.name}':\")\n",
    "\n",
    "        new_name = input(f\"   • Name [{action.name}]: \").strip()\n",
    "        if new_name:\n",
    "            action.name = new_name\n",
    "\n",
    "        new_desc = input(f\"   • Description [{action.description}]: \").strip()\n",
    "        if new_desc:\n",
    "            action.description = new_desc\n",
    "\n",
    "        # If it’s not marked as “sensitive,” allow parameter tweaks\n",
    "        if not action.sensitive_data:\n",
    "            print(\"   • Current parameters:\")\n",
    "            for (k, v) in action.parameters.items():\n",
    "                new_v = input(f\"       {k} [{v}]: \").strip()\n",
    "                if new_v:\n",
    "                    action.parameters[k] = new_v\n",
    "\n",
    "        return action\n",
    "\n",
    "    async def _execute_action(self, action: ActionNode) -> dict:\n",
    "        \"\"\"\n",
    "        Dispatch to the correct Toolkit method based on action.type.\n",
    "        Toolkit should know how to call YouTube, Spotify, ride-booking, banking, etc.\n",
    "        Return a dict { \"status\": \"success\"/\"error\", ... }.\n",
    "        \"\"\"\n",
    "        t = action.type\n",
    "        p = action.parameters\n",
    "\n",
    "        try:\n",
    "            if t == ActionType.MEDIA_PLAY:\n",
    "                # We assume parameters might include \"app\": \"youtube\" or \"spotify\", plus \"query\" or \"track\"\n",
    "                if p.get(\"app\") == \"youtube\":\n",
    "                    return self.tools.youtube_search(p[\"query\"])\n",
    "                elif p.get(\"app\") == \"spotify\":\n",
    "                    return self.tools.spotify_play(p[\"track\"], p.get(\"artist\"))\n",
    "                else:\n",
    "                    # generic media play fallback\n",
    "                    return {\"status\": \"error\", \"error\": \"Unknown media platform\"}\n",
    "\n",
    "            elif t == ActionType.COMMUNICATION:\n",
    "                if p.get(\"type\") == \"sms\":\n",
    "                    return self.tools.send_sms(p[\"recipient\"], p[\"message\"])\n",
    "                elif p.get(\"platform\") == \"whatsapp\":\n",
    "                    return self.tools.whatsapp_send(p[\"contact\"], p[\"message\"])\n",
    "                elif p.get(\"type\") == \"voice_call\":\n",
    "                    return self.tools.make_phone_call(p[\"number\"])\n",
    "                else:\n",
    "                    return {\"status\": \"error\", \"error\": \"Unknown communication subtype\"}\n",
    "\n",
    "            elif t == ActionType.TRANSPORTATION:\n",
    "                # We expect a sequence: open app → set pickup → set destination → book_ride\n",
    "                if p.get(\"action\") == \"book_ride\":\n",
    "                    return self.tools.book_ride(p[\"pickup\"], p[\"destination\"], p.get(\"service\"))\n",
    "                else:\n",
    "                    # For “open app”, “set pickup”, “set destination” steps,\n",
    "                    # we can return a quick “success” since actual data is saved for the final step.\n",
    "                    return {\"status\": \"success\", \"details\": f\"{action.name} done.\"}\n",
    "\n",
    "            elif t == ActionType.FINANCIAL:\n",
    "                # Steps: open banking → authenticate → initiate topup → confirm transaction\n",
    "                if action.name.lower().startswith(\"authenticate\"):\n",
    "                    return {\"status\": \"success\", \"details\": \"Authenticated via PIN/biometric.\"}\n",
    "                elif action.name.lower().startswith(\"initiate\"):\n",
    "                    return self.tools.mobile_banking_topup(\n",
    "                        account=p[\"account_type\"],\n",
    "                        amount=p[\"amount\"],\n",
    "                        pin=p.get(\"pin\", \"\")  # pin might be collected out-of-band\n",
    "                    )\n",
    "                elif action.name.lower().startswith(\"confirm\"):\n",
    "                    return {\"status\": \"success\", \"details\": \"OTP confirmed. Transaction complete.\"}\n",
    "                else:\n",
    "                    return {\"status\": \"success\", \"details\": f\"{action.name} done.\"}\n",
    "\n",
    "            elif t == ActionType.DATA_ENTRY:\n",
    "                # Fill forms in sequential steps. In a true implementation, you'd carry forward the “form_data”\n",
    "                if action.name.lower().startswith(\"open form\"):\n",
    "                    return {\"status\": \"success\", \"details\": \"Form opened.\"}\n",
    "                elif action.name.lower().startswith(\"fill\"):\n",
    "                    return self.tools.fill_share_details(p[\"fields\"])\n",
    "                elif action.name.lower().startswith(\"review\"):\n",
    "                    return {\"status\": \"success\", \"details\": \"Form reviewed & submitted.\"}\n",
    "                else:\n",
    "                    return {\"status\": \"error\", \"error\": \"Unknown data entry step\"}\n",
    "\n",
    "            else:\n",
    "                # Fallback for SYSTEM_CONTROL or WEB_SEARCH\n",
    "                # If it’s “WEB_SEARCH,” assume toolkit has a generic “web_search” method:\n",
    "                if t == ActionType.WEB_SEARCH and p.get(\"query\"):\n",
    "                    return self.tools.web_search(p[\"query\"])\n",
    "                # Otherwise, simply “acknowledge” without doing anything\n",
    "                return {\"status\": \"success\", \"details\": \"No‐op for generic/system control action.\"}\n",
    "\n",
    "        except Exception as exc:\n",
    "            return {\"status\": \"error\", \"error\": str(exc)}\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------------\n",
    "# ENTRY POINT\n",
    "# ------------------------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if __name__ == \"__main__\":\n",
    "    assistant = DailyActivityAssistant()\n",
    "    asyncio.run(assistant.run())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
