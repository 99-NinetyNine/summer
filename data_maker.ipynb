{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The AI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete Training and Evaluation System\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import torch\n",
    "import argparse\n",
    "import logging\n",
    "from pathlib import Path\n",
    "from typing import List, Dict\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "# Advanced Data Augmentation and Preprocessing System\n",
    "import json\n",
    "import base64\n",
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image, ImageEnhance, ImageFilter\n",
    "import io\n",
    "import random\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "import albumentations as A\n",
    "from pathlib import Path\n",
    "import copy\n",
    "import math\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataSet and Data Loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class AdvancedUIDataAugmenter:\n",
    "    \"\"\"Advanced augmentation specifically for UI interaction data\"\"\"\n",
    "    \n",
    "    def __init__(self, preserve_ui_elements: bool = True):\n",
    "        self.preserve_ui_elements = preserve_ui_elements\n",
    "        self.setup_augmentations()\n",
    "    \n",
    "    def setup_augmentations(self):\n",
    "        \"\"\"Setup different types of augmentations\"\"\"\n",
    "        \n",
    "        # Visual augmentations (safe for UI)\n",
    "        self.visual_transforms = A.Compose([\n",
    "            A.RandomBrightnessContrast(\n",
    "                brightness_limit=0.15, \n",
    "                contrast_limit=0.15, \n",
    "                p=0.4\n",
    "            ),\n",
    "            A.HueSaturationValue(\n",
    "                hue_shift_limit=8, \n",
    "                sat_shift_limit=15, \n",
    "                val_shift_limit=15, \n",
    "                p=0.3\n",
    "            ),\n",
    "            A.GaussNoise(var_limit=(5.0, 25.0), p=0.2),\n",
    "            A.RandomGamma(gamma_limit=(85, 115), p=0.3),\n",
    "            A.CLAHE(clip_limit=2.0, tile_grid_size=(4, 4), p=0.2),\n",
    "        ])\n",
    "        \n",
    "        # Geometric augmentations (minimal to preserve click coordinates)\n",
    "        self.geometric_transforms = A.Compose([\n",
    "            A.ShiftScaleRotate(\n",
    "                shift_limit=0.02,  # Very small shifts\n",
    "                scale_limit=0.03,  # Very small scaling\n",
    "                rotate_limit=1,    # Minimal rotation\n",
    "                border_mode=cv2.BORDER_CONSTANT,\n",
    "                value=0,\n",
    "                p=0.3\n",
    "            ),\n",
    "        ])\n",
    "        \n",
    "        # Screen simulation augmentations\n",
    "        self.screen_simulation = [\n",
    "            self._simulate_different_screen_sizes,\n",
    "            self._simulate_different_browsers,\n",
    "            self._simulate_zoom_levels,\n",
    "            self._add_cursor_variations,\n",
    "        ]\n",
    "    \n",
    "    def augment_sequence(self, sequence_data: Dict, num_augmentations: int = 5) -> List[Dict]:\n",
    "        \"\"\"Generate multiple augmented versions of a sequence\"\"\"\n",
    "        augmented_sequences = []\n",
    "        \n",
    "        for i in range(num_augmentations):\n",
    "            # Create a copy of the original sequence\n",
    "            aug_sequence = copy.deepcopy(sequence_data)\n",
    "            \n",
    "            # Apply random augmentations\n",
    "            aug_sequence = self._apply_random_augmentations(aug_sequence)\n",
    "            \n",
    "            # Add variation identifier\n",
    "            aug_sequence['augmentation_id'] = i\n",
    "            aug_sequence['original_task'] = sequence_data.get('task_label', 'unknown')\n",
    "            aug_sequence['task_label'] = f\"{sequence_data.get('task_label', 'unknown')}_aug_{i}\"\n",
    "            \n",
    "            augmented_sequences.append(aug_sequence)\n",
    "        \n",
    "        return augmented_sequences\n",
    "    \n",
    "    def _apply_random_augmentations(self, sequence: Dict) -> Dict:\n",
    "        \"\"\"Apply random augmentations to a sequence\"\"\"\n",
    "        \n",
    "        # Augment screenshots\n",
    "        if 'screenshots' in sequence:\n",
    "            for screenshot in sequence['screenshots']:\n",
    "                screenshot['image_base64'] = self._augment_screenshot(\n",
    "                    screenshot['image_base64']\n",
    "                )\n",
    "        \n",
    "        # Apply action variations\n",
    "        if 'actions' in sequence:\n",
    "            sequence['actions'] = self._augment_actions(sequence['actions'])\n",
    "        \n",
    "        # Apply timing variations\n",
    "        sequence = self._add_timing_variations(sequence)\n",
    "        \n",
    "        return sequence\n",
    "    \n",
    "    def _augment_screenshot(self, base64_image: str) -> str:\n",
    "        \"\"\"Augment a single screenshot\"\"\"\n",
    "        # Decode image\n",
    "        img_data = base64.b64decode(base64_image)\n",
    "        img = Image.open(io.BytesIO(img_data))\n",
    "        img_array = np.array(img)\n",
    "        \n",
    "        # Apply visual augmentations\n",
    "        if random.random() < 0.7:\n",
    "            augmented = self.visual_transforms(image=img_array)\n",
    "            img_array = augmented['image']\n",
    "        \n",
    "        # Apply geometric augmentations (carefully)\n",
    "        if random.random() < 0.3:\n",
    "            augmented = self.geometric_transforms(image=img_array)\n",
    "            img_array = augmented['image']\n",
    "        \n",
    "        # Apply screen simulation\n",
    "        if random.random() < 0.4:\n",
    "            aug_func = random.choice(self.screen_simulation)\n",
    "            img_array = aug_func(img_array)\n",
    "        \n",
    "        # Convert back to base64\n",
    "        img_pil = Image.fromarray(img_array)\n",
    "        buffer = io.BytesIO()\n",
    "        img_pil.save(buffer, format='PNG')\n",
    "        augmented_base64 = base64.b64encode(buffer.getvalue()).decode()\n",
    "        \n",
    "        return augmented_base64\n",
    "    \n",
    "    def _augment_actions(self, actions: List[Dict]) -> List[Dict]:\n",
    "        \"\"\"Add variations to actions\"\"\"\n",
    "        augmented_actions = []\n",
    "        \n",
    "        for action in actions:\n",
    "            aug_action = copy.deepcopy(action)\n",
    "            \n",
    "            # Add small coordinate variations for mouse actions\n",
    "            if action.get('type') == 'mouse' and 'coordinates' in action:\n",
    "                coords = action['coordinates']\n",
    "                if coords:\n",
    "                    # Add small random offset (±5 pixels)\n",
    "                    offset_x = random.randint(-5, 5)\n",
    "                    offset_y = random.randint(-5, 5)\n",
    "                    \n",
    "                    aug_action['coordinates'] = {\n",
    "                        'x': max(0, coords['x'] + offset_x),\n",
    "                        'y': max(0, coords['y'] + offset_y)\n",
    "                    }\n",
    "            \n",
    "            # Add timing variations\n",
    "            if random.random() < 0.3:\n",
    "                time_offset = random.randint(-100, 100)  # ±100ms\n",
    "                aug_action['timestamp_ms'] = max(0, action['timestamp_ms'] + time_offset)\n",
    "            \n",
    "            augmented_actions.append(aug_action)\n",
    "        \n",
    "        return augmented_actions\n",
    "    \n",
    "    def _add_timing_variations(self, sequence: Dict) -> Dict:\n",
    "        \"\"\"Add realistic timing variations\"\"\"\n",
    "        if 'duration_ms' in sequence:\n",
    "            # Add ±10% variation to total duration\n",
    "            variation = int(sequence['duration_ms'] * 0.1)\n",
    "            offset = random.randint(-variation, variation)\n",
    "            sequence['duration_ms'] = max(1000, sequence['duration_ms'] + offset)\n",
    "        \n",
    "        return sequence\n",
    "    \n",
    "    def _simulate_different_screen_sizes(self, image: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Simulate different screen sizes\"\"\"\n",
    "        original_size = image.shape[:2]\n",
    "        \n",
    "        # Common screen sizes\n",
    "        screen_sizes = [\n",
    "            (1366, 768),   # HD\n",
    "            (1920, 1080),  # Full HD\n",
    "            (2560, 1440),  # QHD\n",
    "            (1440, 900),   # MacBook\n",
    "            (1280, 720),   # HD Ready\n",
    "        ]\n",
    "        \n",
    "        target_size = random.choice(screen_sizes)\n",
    "        \n",
    "        # Resize and pad/crop as needed\n",
    "        resized = cv2.resize(image, target_size)\n",
    "        \n",
    "        # If original was larger, we might need to adjust back\n",
    "        if original_size != target_size:\n",
    "            resized = cv2.resize(resized, (original_size[1], original_size[0]))\n",
    "        \n",
    "        return resized\n",
    "    \n",
    "    def _simulate_different_browsers(self, image: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Simulate different browser interfaces\"\"\"\n",
    "        # Add browser-specific UI elements (simplified)\n",
    "        h, w = image.shape[:2]\n",
    "        \n",
    "        # Simulate different browser header heights\n",
    "        header_heights = [60, 80, 100, 120]\n",
    "        header_height = random.choice(header_heights)\n",
    "        \n",
    "        # Add colored header\n",
    "        header_colors = [\n",
    "            [240, 240, 240],  # Light gray (Chrome-like)\n",
    "            [230, 230, 230],  # Slightly darker gray\n",
    "            [250, 250, 250],  # Very light gray\n",
    "            [220, 220, 220],  # Medium gray\n",
    "        ]\n",
    "        \n",
    "        header_color = random.choice(header_colors)\n",
    "        \n",
    "        # Create modified image with browser header simulation\n",
    "        modified_image = image.copy()\n",
    "        if random.random() < 0.3:  # Apply browser simulation occasionally\n",
    "            # Shift content down to simulate browser header\n",
    "            shifted_content = np.zeros_like(modified_image)\n",
    "            shifted_content[header_height:, :] = modified_image[:-header_height, :]\n",
    "            shifted_content[:header_height, :] = header_color\n",
    "            modified_image = shifted_content\n",
    "        \n",
    "        return modified_image\n",
    "    \n",
    "    def _simulate_zoom_levels(self, image: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Simulate different browser zoom levels\"\"\"\n",
    "        zoom_factors = [0.9, 0.95, 1.0, 1.05, 1.1, 1.25]\n",
    "        zoom = random.choice(zoom_factors)\n",
    "        \n",
    "        if zoom != 1.0:\n",
    "            h, w = image.shape[:2]\n",
    "            new_h, new_w = int(h * zoom), int(w * zoom)\n",
    "            \n",
    "            # Resize image\n",
    "            resized = cv2.resize(image, (new_w, new_h))\n",
    "            \n",
    "            # Crop or pad to original size\n",
    "            if zoom > 1.0:  # Crop center\n",
    "                start_y = (new_h - h) // 2\n",
    "                start_x = (new_w - w) // 2\n",
    "                cropped = resized[start_y:start_y+h, start_x:start_x+w]\n",
    "                return cropped\n",
    "            else:  # Pad with background color\n",
    "                padded = np.full((h, w, 3), fill_value=240, dtype=np.uint8)\n",
    "                start_y = (h - new_h) // 2\n",
    "                start_x = (w - new_w) // 2\n",
    "                padded[start_y:start_y+new_h, start_x:start_x+new_w] = resized\n",
    "                return padded\n",
    "        \n",
    "        return image\n",
    "    \n",
    "    def _add_cursor_variations(self, image: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Add different cursor styles (simulated)\"\"\"\n",
    "        # This is a simplified version - in reality, cursors aren't part of screenshots\n",
    "        # But we can simulate their presence for training variety\n",
    "        if random.random() < 0.2:  # Rarely add cursor simulation\n",
    "            h, w = image.shape[:2]\n",
    "            \n",
    "            # Random cursor position\n",
    "            cursor_x = random.randint(50, w - 50)\n",
    "            cursor_y = random.randint(50, h - 50)\n",
    "            \n",
    "            # Add small cursor-like mark (just a few pixels)\n",
    "            cursor_color = [0, 0, 0]  # Black cursor\n",
    "            image[cursor_y:cursor_y+2, cursor_x:cursor_x+2] = cursor_color\n",
    "        \n",
    "        return image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class TaskVariationGenerator:\n",
    "    \"\"\"Generate variations of tasks to increase dataset diversity\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.task_templates = {\n",
    "            'login': {\n",
    "                'variations': [\n",
    "                    'login_gmail', 'login_facebook', 'login_twitter', \n",
    "                    'login_github', 'login_linkedin', 'login_microsoft', \"L\"\n",
    "                ],\n",
    "                'common_elements': ['email_field', 'password_field', 'login_button']\n",
    "            },\n",
    "            'search': {\n",
    "                'variations': [\n",
    "                    'search_google', 'search_youtube', 'search_amazon',\n",
    "                    'search_stackoverflow', 'search_github'\n",
    "                ],\n",
    "                'common_elements': ['search_box', 'search_button', 'results']\n",
    "            },\n",
    "            'form_fill': {\n",
    "                'variations': [\n",
    "                    'contact_form', 'registration_form', 'survey_form',\n",
    "                    'checkout_form', 'profile_form'\n",
    "                ],\n",
    "                'common_elements': ['text_fields', 'dropdown', 'submit_button']\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    def generate_task_variations(self, original_sequence: Dict) -> List[Dict]:\n",
    "        \"\"\"Generate task variations based on the original task\"\"\"\n",
    "        base_task = original_sequence.get('task_label', 'unknown')\n",
    "        variations = []\n",
    "        \n",
    "        # Find matching task template\n",
    "        task_type = self._identify_task_type(base_task)\n",
    "        \n",
    "        if task_type in self.task_templates:\n",
    "            template = self.task_templates[task_type]\n",
    "            \n",
    "            for variation_name in template['variations']:\n",
    "                if variation_name != base_task:  # Don't duplicate original\n",
    "                    varied_sequence = copy.deepcopy(original_sequence)\n",
    "                    varied_sequence['task_label'] = variation_name\n",
    "                    varied_sequence['task_variation'] = True\n",
    "                    varied_sequence['base_task'] = base_task\n",
    "                    variations.append(varied_sequence)\n",
    "        \n",
    "        return variations\n",
    "    \n",
    "    def _identify_task_type(self, task_label: str) -> str:\n",
    "        \"\"\"Identify the general type of task\"\"\"\n",
    "        task_lower = task_label.lower()\n",
    "        \n",
    "        if 'login' in task_lower or 'signin' in task_lower:\n",
    "            return 'login'\n",
    "        elif 'search' in task_lower:\n",
    "            return 'search'\n",
    "        elif 'form' in task_lower or 'fill' in task_lower:\n",
    "            return 'form_fill'\n",
    "        \n",
    "        return 'unknown'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class SequentialActionGenerator:\n",
    "    \"\"\"Generate realistic sequential actions for training\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.action_sequences = {\n",
    "            'typing': self._generate_typing_sequence,\n",
    "            'navigation': self._generate_navigation_sequence,\n",
    "            'form_interaction': self._generate_form_sequence,\n",
    "        }\n",
    "    \n",
    "    def generate_intermediate_actions(self, sequence: Dict) -> Dict:\n",
    "        \"\"\"Generate intermediate actions to make sequences more realistic\"\"\"\n",
    "        actions = sequence.get('actions', [])\n",
    "        if len(actions) < 2:\n",
    "            return sequence\n",
    "        \n",
    "        enhanced_actions = []\n",
    "        \n",
    "        for i, action in enumerate(actions):\n",
    "            enhanced_actions.append(action)\n",
    "            \n",
    "            # Add intermediate actions between major actions\n",
    "            if i < len(actions) - 1:\n",
    "                next_action = actions[i + 1]\n",
    "                intermediate = self._generate_intermediate_action(action, next_action)\n",
    "                if intermediate:\n",
    "                    enhanced_actions.extend(intermediate)\n",
    "        \n",
    "        sequence['actions'] = enhanced_actions\n",
    "        return sequence\n",
    "    \n",
    "    def _generate_intermediate_action(self, current: Dict, next_action: Dict) -> List[Dict]:\n",
    "        \"\"\"Generate realistic intermediate actions\"\"\"\n",
    "        intermediate_actions = []\n",
    "        \n",
    "        # Add mouse movement before clicks\n",
    "        if (next_action.get('type') == 'mouse' and \n",
    "            next_action.get('action') == 'click' and \n",
    "            current.get('type') != 'mouse'):\n",
    "            \n",
    "            mouse_move = {\n",
    "                'timestamp_ms': current['timestamp_ms'] + 50,\n",
    "                'type': 'mouse',\n",
    "                'action': 'move',\n",
    "                'coordinates': next_action.get('coordinates', {'x': 0, 'y': 0})\n",
    "            }\n",
    "            intermediate_actions.append(mouse_move)\n",
    "        \n",
    "        # Add pauses between rapid actions\n",
    "        time_diff = next_action['timestamp_ms'] - current['timestamp_ms']\n",
    "        if time_diff < 100:  # Very fast actions\n",
    "            pause_action = {\n",
    "                'timestamp_ms': current['timestamp_ms'] + time_diff // 2,\n",
    "                'type': 'system',\n",
    "                'action': 'pause',\n",
    "                'duration': 50\n",
    "            }\n",
    "            intermediate_actions.append(pause_action)\n",
    "        \n",
    "        return intermediate_actions\n",
    "    \n",
    "    def _generate_typing_sequence(self, text: str, start_time: int) -> List[Dict]:\n",
    "        \"\"\"Generate realistic typing sequence\"\"\"\n",
    "        actions = []\n",
    "        current_time = start_time\n",
    "        \n",
    "        for char in text:\n",
    "            # Random typing speed (50-200ms per character)\n",
    "            char_delay = random.randint(50, 200)\n",
    "            \n",
    "            # Key press\n",
    "            actions.append({\n",
    "                'timestamp_ms': current_time,\n",
    "                'type': 'keyboard',\n",
    "                'action': 'press',\n",
    "                'key': char,\n",
    "                'key_code': ord(char)\n",
    "            })\n",
    "            \n",
    "            # Key release\n",
    "            actions.append({\n",
    "                'timestamp_ms': current_time + random.randint(20, 80),\n",
    "                'type': 'keyboard',\n",
    "                'action': 'release',\n",
    "                'key': char\n",
    "            })\n",
    "            \n",
    "            current_time += char_delay\n",
    "        \n",
    "        return actions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class DatasetEnhancer:\n",
    "    \"\"\"Main class to enhance and expand the dataset\"\"\"\n",
    "    \n",
    "    def __init__(self, output_dir: str = \"enhanced_dataset\"):\n",
    "        self.output_dir = Path(output_dir)\n",
    "        self.output_dir.mkdir(exist_ok=True)\n",
    "        \n",
    "        self.augmenter = AdvancedUIDataAugmenter()\n",
    "        self.task_generator = TaskVariationGenerator()\n",
    "        self.action_generator = SequentialActionGenerator()\n",
    "    \n",
    "    def enhance_dataset(self, input_files: List[str], enhancement_factor: int = 5) -> List[str]:\n",
    "        \"\"\"Enhance dataset with multiple augmentation techniques\"\"\"\n",
    "        enhanced_files = []\n",
    "        \n",
    "        for input_file in input_files:\n",
    "            print(f\"Processing {input_file}...\")\n",
    "            \n",
    "            # Load original data\n",
    "            with open(input_file, 'r') as f:\n",
    "                original_data = json.load(f)\n",
    "            \n",
    "            enhanced_sequences = []\n",
    "            \n",
    "            # 1. Visual and timing augmentations\n",
    "            visual_augmented = self.augmenter.augment_sequence(\n",
    "                original_data, num_augmentations=enhancement_factor\n",
    "            )\n",
    "            enhanced_sequences.extend(visual_augmented)\n",
    "            \n",
    "            # 2. Task variations\n",
    "            task_variations = self.task_generator.generate_task_variations(original_data)\n",
    "            enhanced_sequences.extend(task_variations)\n",
    "            \n",
    "            # 3. Action sequence enhancements\n",
    "            for seq in enhanced_sequences[:]:  # Copy list to avoid modifying during iteration\n",
    "                enhanced_seq = self.action_generator.generate_intermediate_actions(seq)\n",
    "                enhanced_sequences.append(enhanced_seq)\n",
    "            \n",
    "            # Save enhanced sequences\n",
    "            for i, enhanced_seq in enumerate(enhanced_sequences):\n",
    "                output_filename = f\"{Path(input_file).stem}_enhanced_{i}.json\"\n",
    "                output_path = self.output_dir / output_filename\n",
    "                \n",
    "                with open(output_path, 'w') as f:\n",
    "                    json.dump(enhanced_seq, f, indent=2)\n",
    "                \n",
    "                enhanced_files.append(str(output_path))\n",
    "        \n",
    "        print(f\"Generated {len(enhanced_files)} enhanced data files\")\n",
    "        return enhanced_files\n",
    "    \n",
    "    def create_balanced_dataset(self, input_files: List[str]) -> str:\n",
    "        \"\"\"Create a balanced dataset with equal representation\"\"\"\n",
    "        all_sequences = []\n",
    "        task_counts = {}\n",
    "        \n",
    "        # Load all sequences and count tasks\n",
    "        for file_path in input_files:\n",
    "            with open(file_path, 'r') as f:\n",
    "                data = json.load(f)\n",
    "                task_label = data.get('task_label', 'unknown')\n",
    "                \n",
    "                if task_label not in task_counts:\n",
    "                    task_counts[task_label] = []\n",
    "                \n",
    "                task_counts[task_label].append(data)\n",
    "        \n",
    "        # Balance dataset\n",
    "        max_samples = max(len(sequences) for sequences in task_counts.values())\n",
    "        balanced_sequences = []\n",
    "        \n",
    "        for task_label, sequences in task_counts.items():\n",
    "            # Upsample if needed\n",
    "            while len(sequences) < max_samples:\n",
    "                # Randomly select and augment existing sequences\n",
    "                base_seq = random.choice(sequences)\n",
    "                augmented = self.augmenter.augment_sequence(base_seq, num_augmentations=1)[0]\n",
    "                sequences.append(augmented)\n",
    "            \n",
    "            balanced_sequences.extend(sequences[:max_samples])\n",
    "        \n",
    "        # Save balanced dataset\n",
    "        balanced_file = self.output_dir / \"balanced_dataset.json\"\n",
    "        with open(balanced_file, 'w') as f:\n",
    "            json.dump(balanced_sequences, f, indent=2)\n",
    "        \n",
    "        print(f\"Created balanced dataset with {len(balanced_sequences)} sequences\")\n",
    "        print(f\"Task distribution: {dict((k, len(v)) for k, v in task_counts.items())}\")\n",
    "        \n",
    "        return str(balanced_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tracker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete Training and Evaluation System\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import torch\n",
    "import argparse\n",
    "import logging\n",
    "from pathlib import Path\n",
    "from typing import List, Dict\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import numpy as np\n",
    "from datetime import datetime\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ExperimentTracker:\n",
    "    \"\"\"Track and log experiments\"\"\"\n",
    "    \n",
    "    def __init__(self, experiment_name: str, use_wandb: bool = False):\n",
    "        self.experiment_name = experiment_name\n",
    "        self.use_wandb = use_wandb\n",
    "        self.metrics_history = []\n",
    "        \n",
    "        # Setup logging\n",
    "        logging.basicConfig(\n",
    "            level=logging.INFO,\n",
    "            format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "            handlers=[\n",
    "                logging.FileHandler(f'{experiment_name}_training.log'),\n",
    "                logging.StreamHandler()\n",
    "            ]\n",
    "        )\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "        \n",
    "        # Initialize wandb if requested\n",
    "        if self.use_wandb:\n",
    "            try:\n",
    "                wandb.init(project=\"ui-action-prediction\", name=experiment_name)\n",
    "            except Exception as e:\n",
    "                self.logger.warning(f\"Failed to initialize wandb: {e}\")\n",
    "                self.use_wandb = False\n",
    "    \n",
    "    def log_metrics(self, metrics: Dict, step: int):\n",
    "        \"\"\"Log metrics to file and wandb\"\"\"\n",
    "        metrics['step'] = step\n",
    "        metrics['timestamp'] = datetime.now().isoformat()\n",
    "        self.metrics_history.append(metrics)\n",
    "        \n",
    "        # Log to console\n",
    "        self.logger.info(f\"Step {step}: {metrics}\")\n",
    "        \n",
    "        # Log to wandb\n",
    "        if self.use_wandb:\n",
    "            wandb.log(metrics, step=step)\n",
    "    \n",
    "    def save_metrics(self, filepath: str):\n",
    "        \"\"\"Save metrics history to file\"\"\"\n",
    "        with open(filepath, 'w') as f:\n",
    "            json.dump(self.metrics_history, f, indent=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ModelEvaluator:\n",
    "    \"\"\"Comprehensive model evaluation\"\"\"\n",
    "    \n",
    "    def __init__(self, model, dataset, action_encoder, device='cuda'):\n",
    "        self.model = model\n",
    "        self.dataset = dataset\n",
    "        self.action_encoder = action_encoder\n",
    "        self.device = device\n",
    "    \n",
    "    def evaluate_comprehensive(self, dataloader) -> Dict:\n",
    "        \"\"\"Comprehensive evaluation including per-class metrics\"\"\"\n",
    "        self.model.eval()\n",
    "        \n",
    "        all_predictions = []\n",
    "        all_targets = []\n",
    "        all_coordinates_pred = []\n",
    "        all_coordinates_true = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in dataloader:\n",
    "                images = batch['image'].to(self.device)\n",
    "                action_targets = batch['action_class'].squeeze().to(self.device)\n",
    "                coord_targets = batch['coordinates'].to(self.device)\n",
    "                \n",
    "                action_logits, coord_pred = self.model(images)\n",
    "                action_pred = torch.argmax(action_logits, dim=1)\n",
    "                \n",
    "                all_predictions.extend(action_pred.cpu().numpy())\n",
    "                all_targets.extend(action_targets.cpu().numpy())\n",
    "                all_coordinates_pred.extend(coord_pred.cpu().numpy())\n",
    "                all_coordinates_true.extend(coord_targets.cpu().numpy())\n",
    "        \n",
    "        # Calculate metrics\n",
    "        metrics = {}\n",
    "        \n",
    "        # Action classification metrics\n",
    "        metrics['classification_report'] = classification_report(\n",
    "            all_targets, all_predictions, \n",
    "            target_names=self.action_encoder.classes_,\n",
    "            output_dict=True\n",
    "        )\n",
    "        \n",
    "        # Coordinate regression metrics\n",
    "        coord_pred = np.array(all_coordinates_pred)\n",
    "        coord_true = np.array(all_coordinates_true)\n",
    "        \n",
    "        metrics['coordinate_mae'] = np.mean(np.abs(coord_pred - coord_true), axis=0)\n",
    "        metrics['coordinate_rmse'] = np.sqrt(np.mean((coord_pred - coord_true)**2, axis=0))\n",
    "        \n",
    "        # Pixel accuracy (within N pixels)\n",
    "        pixel_thresholds = [5, 10, 20, 50]\n",
    "        for threshold in pixel_thresholds:\n",
    "            pixel_distances = np.sqrt(np.sum((coord_pred - coord_true)**2, axis=1))\n",
    "            accuracy = np.mean(pixel_distances <= threshold/1920.0)  # Normalized\n",
    "            metrics[f'pixel_accuracy_{threshold}px'] = accuracy\n",
    "        \n",
    "        return metrics\n",
    "    \n",
    "    def plot_confusion_matrix(self, dataloader, save_path: str):\n",
    "        \"\"\"Generate and save confusion matrix\"\"\"\n",
    "        self.model.eval()\n",
    "        all_predictions = []\n",
    "        all_targets = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in dataloader:\n",
    "                images = batch['image'].to(self.device)\n",
    "                action_targets = batch['action_class'].squeeze().to(self.device)\n",
    "                \n",
    "                action_logits, _ = self.model(images)\n",
    "                action_pred = torch.argmax(action_logits, dim=1)\n",
    "                \n",
    "                all_predictions.extend(action_pred.cpu().numpy())\n",
    "                all_targets.extend(action_targets.cpu().numpy())\n",
    "        \n",
    "        # Create confusion matrix\n",
    "        cm = confusion_matrix(all_targets, all_predictions)\n",
    "        \n",
    "        plt.figure(figsize=(12, 10))\n",
    "        sns.heatmap(cm, annot=True, fmt='d', \n",
    "                   xticklabels=self.action_encoder.classes_,\n",
    "                   yticklabels=self.action_encoder.classes_)\n",
    "        plt.title('Action Prediction Confusion Matrix')\n",
    "        plt.ylabel('True Action')\n",
    "        plt.xlabel('Predicted Action')\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.yticks(rotation=45)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "    \n",
    "    def analyze_failure_cases(self, dataloader, num_samples: int = 20) -> List[Dict]:\n",
    "        \"\"\"Analyze failure cases for debugging\"\"\"\n",
    "        self.model.eval()\n",
    "        failure_cases = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in dataloader:\n",
    "                images = batch['image'].to(self.device)\n",
    "                action_targets = batch['action_class'].squeeze().to(self.device)\n",
    "                coord_targets = batch['coordinates'].to(self.device)\n",
    "                \n",
    "                action_logits, coord_pred = self.model(images)\n",
    "                action_pred = torch.argmax(action_logits, dim=1)\n",
    "                \n",
    "                # Find incorrect predictions\n",
    "                incorrect_mask = action_pred != action_targets\n",
    "                \n",
    "                if incorrect_mask.any():\n",
    "                    for i in torch.where(incorrect_mask)[0]:\n",
    "                        if len(failure_cases) >= num_samples:\n",
    "                            break\n",
    "                        \n",
    "                        failure_case = {\n",
    "                            'predicted_action': self.action_encoder.classes_[action_pred[i]],\n",
    "                            'true_action': self.action_encoder.classes_[action_targets[i]],\n",
    "                            'predicted_coords': coord_pred[i].cpu().numpy().tolist(),\n",
    "                            'true_coords': coord_targets[i].cpu().numpy().tolist(),\n",
    "                            'confidence': torch.softmax(action_logits[i], dim=0).max().item()\n",
    "                        }\n",
    "                        failure_cases.append(failure_case)\n",
    "                \n",
    "                if len(failure_cases) >= num_samples:\n",
    "                    break\n",
    "        \n",
    "        return failure_cases\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class AdvancedTrainingPipeline:\n",
    "    \"\"\"Advanced training pipeline with all features\"\"\"\n",
    "    \n",
    "    def __init__(self, config: Dict):\n",
    "        self.config = config\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        \n",
    "        # Initialize experiment tracker\n",
    "        self.tracker = ExperimentTracker(\n",
    "            config['experiment_name'])\n",
    "        \n",
    "        # Save config template\n",
    "        config_path = f\"{config['experiment_name']}_config.json\"\n",
    "        with open(config_path, 'w') as f:\n",
    "            json.dump(config, f, indent=2)\n",
    "        print(f\"Created config template: {config_path}\")\n",
    "    \n",
    "    # # Run training pipeline\n",
    "    # pipeline = AdvancedTrainingPipeline(config)\n",
    "    # model_path = pipeline.train()\n",
    "    \n",
    "    # print(f\"\\nTraining completed!\")\n",
    "    # print(f\"Best model saved to: {model_path}\")\n",
    "    \n",
    "    # # Demonstrate real-time execution\n",
    "    # print(\"\\nStarting demonstration of trained model...\")\n",
    "    # demonstrate_execution(model_path, config)\n",
    "\n",
    "def demonstrate_execution(model_path: str, config: Dict):\n",
    "    \"\"\"Demonstrate the trained model in action\"\"\"\n",
    "    try:\n",
    "        # Initialize execution agent\n",
    "        encoder_path = str(Path(model_path).parent / 'action_encoder.pkl')\n",
    "        agent = RealTimeExecutionAgent(model_path, encoder_path)\n",
    "        \n",
    "        print(\"Real-time execution agent initialized!\")\n",
    "        print(\"The agent can now predict and execute actions based on screen content.\")\n",
    "        print(\"To run automation, call: agent.run_automation('task_name')\")\n",
    "        \n",
    "        # Demo: Just predict without executing\n",
    "        screenshot = agent.capture_screen()\n",
    "        predicted_action = agent.predict_action(screenshot)\n",
    "        \n",
    "        print(f\"Demo prediction from current screen:\")\n",
    "        print(f\"  Action: {predicted_action['type']} - {predicted_action['action']}\")\n",
    "        print(f\"  Coordinates: {predicted_action['coordinates']}\")\n",
    "        print(f\"  Confidence: {predicted_action['confidence']:.3f}\")\n",
    "        \n",
    "        if predicted_action['key']:\n",
    "            print(f\"  Key: {predicted_action['key']}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Demo execution failed: {e}\")\n",
    "\n",
    "class LiveTrainingMonitor:\n",
    "    \"\"\"Monitor training progress in real-time\"\"\"\n",
    "    \n",
    "    def __init__(self, log_file: str):\n",
    "        self.log_file = log_file\n",
    "        self.metrics = []\n",
    "    \n",
    "    def plot_training_progress(self, save_path: str = \"training_progress.png\"):\n",
    "        \"\"\"Plot training progress\"\"\"\n",
    "        if not self.metrics:\n",
    "            return\n",
    "        \n",
    "        epochs = [m['epoch'] for m in self.metrics]\n",
    "        train_loss = [m['train_loss'] for m in self.metrics]\n",
    "        val_loss = [m['val_loss'] for m in self.metrics]\n",
    "        val_accuracy = [m['val_accuracy'] for m in self.metrics]\n",
    "        \n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "        \n",
    "        # Loss plot\n",
    "        ax1.plot(epochs, train_loss, label='Train Loss', color='blue')\n",
    "        ax1.plot(epochs, val_loss, label='Validation Loss', color='red')\n",
    "        ax1.set_xlabel('Epoch')\n",
    "        ax1.set_ylabel('Loss')\n",
    "        ax1.set_title('Training and Validation Loss')\n",
    "        ax1.legend()\n",
    "        ax1.grid(True)\n",
    "        \n",
    "        # Accuracy plot\n",
    "        ax2.plot(epochs, val_accuracy, label='Validation Accuracy', color='green')\n",
    "        ax2.set_xlabel('Epoch')\n",
    "        ax2.set_ylabel('Accuracy')\n",
    "        ax2.set_title('Validation Accuracy')\n",
    "        ax2.legend()\n",
    "        ax2.grid(True)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "\n",
    "# Additional utility functions\n",
    "\n",
    "def validate_data_format(data_file: str) -> bool:\n",
    "    \"\"\"Validate that data file has correct format\"\"\"\n",
    "    try:\n",
    "        with open(data_file, 'r') as f:\n",
    "            data = json.load(f)\n",
    "        \n",
    "        required_fields = ['task_label', 'screenshots', 'actions']\n",
    "        for field in required_fields:\n",
    "            if field not in data:\n",
    "                print(f\"Missing required field: {field}\")\n",
    "                return False\n",
    "        \n",
    "        # Validate screenshots\n",
    "        if not data['screenshots']:\n",
    "            print(\"No screenshots found\")\n",
    "            return False\n",
    "            \n",
    "        for screenshot in data['screenshots'][:1]:  # Check first screenshot\n",
    "            if 'image_base64' not in screenshot:\n",
    "                print(\"Screenshot missing image_base64\")\n",
    "                return False\n",
    "        \n",
    "        # Validate actions\n",
    "        if not data['actions']:\n",
    "            print(\"No actions found\")\n",
    "            return False\n",
    "            \n",
    "        for action in data['actions'][:1]:  # Check first action\n",
    "            if 'type' not in action or 'action' not in action:\n",
    "                print(\"Action missing required fields\")\n",
    "                return False\n",
    "        \n",
    "        print(f\"Data file {data_file} is valid!\")\n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error validating {data_file}: {e}\")\n",
    "        return False\n",
    "\n",
    "def setup_environment():\n",
    "    \"\"\"Setup the training environment\"\"\"\n",
    "    print(\"Setting up UI Action Prediction environment...\")\n",
    "    \n",
    "    # Create necessary directories\n",
    "    directories = [\n",
    "        'data', 'models', 'experiments', 'logs', \n",
    "        'enhanced_data', 'plots', 'configs'\n",
    "    ]\n",
    "    \n",
    "    for dir_name in directories:\n",
    "        Path(dir_name).mkdir(exist_ok=True)\n",
    "    \n",
    "    # Create sample config\n",
    "    config = create_config_template()\n",
    "    with open('configs/default_config.json', 'w') as f:\n",
    "        json.dump(config, f, indent=2)\n",
    "    \n",
    "    # Create sample data structure\n",
    "    sample_data = create_sample_data_structure()\n",
    "    with open('data/sample_data_structure.json', 'w') as f:\n",
    "        json.dump(sample_data, f, indent=2)\n",
    "    \n",
    "    print(\"Environment setup complete!\")\n",
    "    print(\"Created directories:\", directories)\n",
    "    print(\"Sample config: configs/default_config.json\")\n",
    "    print(\"Sample data structure: data/sample_data_structure.json\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Check if setup is needed\n",
    "    if len(sys.argv) > 1 and sys.argv[1] == 'setup':\n",
    "        setup_environment()\n",
    "        sys.exit(0)\n",
    "    \n",
    "    # Run main training\n",
    "    main()experiment_name'], \n",
    "            config.get('use_wandb', False)\n",
    "        )\n",
    "        \n",
    "        # Create output directories\n",
    "        self.output_dir = Path(config['output_dir'])\n",
    "        self.output_dir.mkdir(exist_ok=True)\n",
    "        (self.output_dir / 'models').mkdir(exist_ok=True)\n",
    "        (self.output_dir / 'plots').mkdir(exist_ok=True)\n",
    "        (self.output_dir / 'logs').mkdir(exist_ok=True)\n",
    "    \n",
    "    def prepare_data(self) -> Tuple[DataLoader, DataLoader, DataLoader]:\n",
    "        \"\"\"Prepare training, validation, and test data\"\"\"\n",
    "        self.tracker.logger.info(\"Preparing dataset...\")\n",
    "        \n",
    "        # Enhance dataset if requested\n",
    "        if self.config.get('enhance_data', True):\n",
    "            enhancer = DatasetEnhancer(str(self.output_dir / 'enhanced_data'))\n",
    "            enhanced_files = enhancer.enhance_dataset(\n",
    "                self.config['data_files'], \n",
    "                enhancement_factor=self.config.get('enhancement_factor', 5)\n",
    "            )\n",
    "            data_files = enhanced_files\n",
    "        else:\n",
    "            data_files = self.config['data_files']\n",
    "        \n",
    "        # Create dataset\n",
    "        from data_augmentation import AdvancedUIDataAugmenter\n",
    "        augmenter = AdvancedUIDataAugmenter()\n",
    "        \n",
    "        dataset = UIActionDataset(\n",
    "            data_files, \n",
    "            transform=augmenter.visual_transforms,\n",
    "            augment=True\n",
    "        )\n",
    "        \n",
    "        # Split dataset\n",
    "        total_size = len(dataset)\n",
    "        train_size = int(0.7 * total_size)\n",
    "        val_size = int(0.15 * total_size)\n",
    "        test_size = total_size - train_size - val_size\n",
    "        \n",
    "        train_dataset, val_dataset, test_dataset = torch.utils.data.random_split(\n",
    "            dataset, [train_size, val_size, test_size]\n",
    "        )\n",
    "        \n",
    "        # Create dataloaders\n",
    "        train_loader = DataLoader(\n",
    "            train_dataset, \n",
    "            batch_size=self.config['batch_size'], \n",
    "            shuffle=True,\n",
    "            num_workers=4,\n",
    "            pin_memory=True\n",
    "        )\n",
    "        val_loader = DataLoader(\n",
    "            val_dataset, \n",
    "            batch_size=self.config['batch_size'], \n",
    "            shuffle=False,\n",
    "            num_workers=4,\n",
    "            pin_memory=True\n",
    "        )\n",
    "        test_loader = DataLoader(\n",
    "            test_dataset, \n",
    "            batch_size=self.config['batch_size'], \n",
    "            shuffle=False,\n",
    "            num_workers=4,\n",
    "            pin_memory=True\n",
    "        )\n",
    "        \n",
    "        self.action_encoder = dataset.action_encoder\n",
    "        self.tracker.logger.info(f\"Dataset prepared: {len(dataset)} total samples\")\n",
    "        self.tracker.logger.info(f\"Train: {len(train_dataset)}, Val: {len(val_dataset)}, Test: {len(test_dataset)}\")\n",
    "        \n",
    "        return train_loader, val_loader, test_loader\n",
    "    \n",
    "    def create_model(self) -> ActionPredictionModel:\n",
    "        \"\"\"Create and initialize model\"\"\"\n",
    "        num_classes = len(self.action_encoder.classes_)\n",
    "        model = ActionPredictionModel(\n",
    "            num_classes, \n",
    "            input_size=tuple(self.config.get('input_size', [224, 224]))\n",
    "        )\n",
    "        \n",
    "        # Load pretrained weights if specified\n",
    "        if self.config.get('pretrained_path'):\n",
    "            checkpoint = torch.load(self.config['pretrained_path'])\n",
    "            model.load_state_dict(checkpoint['model_state_dict'])\n",
    "            self.tracker.logger.info(f\"Loaded pretrained model from {self.config['pretrained_path']}\")\n",
    "        \n",
    "        return model.to(self.device)\n",
    "    \n",
    "    def train(self):\n",
    "        \"\"\"Complete training pipeline\"\"\"\n",
    "        self.tracker.logger.info(\"Starting training pipeline...\")\n",
    "        \n",
    "        # Prepare data\n",
    "        train_loader, val_loader, test_loader = self.prepare_data()\n",
    "        \n",
    "        # Create model\n",
    "        model = self.create_model()\n",
    "        \n",
    "        # Create trainer\n",
    "        trainer = UIActionTrainer(model, self.device)\n",
    "        \n",
    "        # Training parameters\n",
    "        num_epochs = self.config.get('num_epochs', 100)\n",
    "        early_stopping_patience = self.config.get('early_stopping_patience', 15)\n",
    "        best_val_accuracy = 0\n",
    "        patience_counter = 0\n",
    "        \n",
    "        # Training loop\n",
    "        for epoch in range(num_epochs):\n",
    "            self.tracker.logger.info(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "            \n",
    "            # Train\n",
    "            train_metrics = trainer.train_epoch(train_loader)\n",
    "            \n",
    "            # Validate\n",
    "            val_metrics = trainer.validate(val_loader)\n",
    "            \n",
    "            # Combine metrics\n",
    "            combined_metrics = {\n",
    "                'epoch': epoch + 1,\n",
    "                'train_loss': train_metrics['total_loss'],\n",
    "                'train_action_loss': train_metrics['action_loss'],\n",
    "                'train_coord_loss': train_metrics['coord_loss'],\n",
    "                'val_loss': val_metrics['total_loss'],\n",
    "                'val_accuracy': val_metrics['action_accuracy'],\n",
    "                'val_coord_mae': val_metrics['coord_mae']\n",
    "            }\n",
    "            \n",
    "            # Log metrics\n",
    "            self.tracker.log_metrics(combined_metrics, epoch + 1)\n",
    "            \n",
    "            # Save best model\n",
    "            if val_metrics['action_accuracy'] > best_val_accuracy:\n",
    "                best_val_accuracy = val_metrics['action_accuracy']\n",
    "                patience_counter = 0\n",
    "                \n",
    "                # Save model\n",
    "                model_path = self.output_dir / 'models' / 'best_model.pth'\n",
    "                torch.save({\n",
    "                    'epoch': epoch,\n",
    "                    'model_state_dict': model.state_dict(),\n",
    "                    'optimizer_state_dict': trainer.optimizer.state_dict(),\n",
    "                    'val_accuracy': best_val_accuracy,\n",
    "                    'config': self.config\n",
    "                }, model_path)\n",
    "                \n",
    "                # Save action encoder\n",
    "                encoder_path = self.output_dir / 'models' / 'action_encoder.pkl'\n",
    "                import pickle\n",
    "                with open(encoder_path, 'wb') as f:\n",
    "                    pickle.dump(self.action_encoder, f)\n",
    "                \n",
    "                self.tracker.logger.info(f\"Saved best model with accuracy: {best_val_accuracy:.4f}\")\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "            \n",
    "            # Early stopping\n",
    "            if patience_counter >= early_stopping_patience:\n",
    "                self.tracker.logger.info(f\"Early stopping at epoch {epoch+1}\")\n",
    "                break\n",
    "        \n",
    "        # Final evaluation\n",
    "        self.tracker.logger.info(\"Starting final evaluation...\")\n",
    "        evaluator = ModelEvaluator(model, None, self.action_encoder, self.device)\n",
    "        \n",
    "        # Comprehensive evaluation\n",
    "        test_metrics = evaluator.evaluate_comprehensive(test_loader)\n",
    "        \n",
    "        # Plot confusion matrix\n",
    "        evaluator.plot_confusion_matrix(\n",
    "            test_loader, \n",
    "            str(self.output_dir / 'plots' / 'confusion_matrix.png')\n",
    "        )\n",
    "        \n",
    "        # Analyze failure cases\n",
    "        failure_cases = evaluator.analyze_failure_cases(test_loader)\n",
    "        \n",
    "        # Save evaluation results\n",
    "        evaluation_results = {\n",
    "            'test_metrics': test_metrics,\n",
    "            'failure_cases': failure_cases,\n",
    "            'final_config': self.config\n",
    "        }\n",
    "        \n",
    "        with open(self.output_dir / 'logs' / 'evaluation_results.json', 'w') as f:\n",
    "            json.dump(evaluation_results, f, indent=2, default=str)\n",
    "        \n",
    "        # Save metrics history\n",
    "        self.tracker.save_metrics(str(self.output_dir / 'logs' / 'training_metrics.json'))\n",
    "        \n",
    "        self.tracker.logger.info(\"Training pipeline completed!\")\n",
    "        self.tracker.logger.info(f\"Best validation accuracy: {best_val_accuracy:.4f}\")\n",
    "        self.tracker.logger.info(f\"Results saved to: {self.output_dir}\")\n",
    "        \n",
    "        return str(self.output_dir / 'models' / 'best_model.pth')\n",
    "\n",
    "def create_config_template() -> Dict:\n",
    "    \"\"\"Create a configuration template\"\"\"\n",
    "    return {\n",
    "        'experiment_name': 'ui_action_experiment_1',\n",
    "        'data_files': ['login_task_data.json'],  # List of your data files\n",
    "        'output_dir': 'experiments/exp_1',\n",
    "        'batch_size': 16,\n",
    "        'num_epochs': 100,\n",
    "        'early_stopping_patience': 15,\n",
    "        'input_size': [224, 224],\n",
    "        'enhance_data': True,\n",
    "        'enhancement_factor': 8,\n",
    "        'use_wandb': False,  # Set to True if you want to use Weights & Biases\n",
    "        'pretrained_path': None  # Path to pretrained model if available\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser(description='UI Action Prediction Training')\n",
    "    parser.add_argument('--config', type=str, help='Path to config file')\n",
    "    parser.add_argument('--data-dir', type=str, help='Directory containing data files')\n",
    "    parser.add_argument('--experiment-name', type=str, default='ui_action_exp', help='Experiment name')\n",
    "    \n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    # Load or create config\n",
    "    if args.config and os.path.exists(args.config):\n",
    "        with open(args.config, 'r') as f:\n",
    "            config = json.load(f)\n",
    "    else:\n",
    "        config = create_config_template()\n",
    "        \n",
    "        # Update with command line arguments\n",
    "        if args.data_dir:\n",
    "            data_files = list(Path(args.data_dir).glob('*.json'))\n",
    "            config['data_files'] = [str(f) for f in data_files]\n",
    "        \n",
    "        config['"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
